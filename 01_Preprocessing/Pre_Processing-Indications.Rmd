---
title: "Evaluate Inclusion Criteria"
output: html_notebook
---
Of the used 1.1M entries, only a subset has subsequent hospital episode (in patient) information. Check the inclusion criteria to exclude as little samples as possible

## Load libraries and data
```{r}
library(tidyverse)
library(lubridate) 
```

Load the data
```{r}
data_base_path <- "../00_Data/"

# Specify output sub folder
output_path <- paste(data_base_path, "test_set", sep = "/")

rxid_with_admission_path <- paste0(data_base_path, "RxID_within_admissions_all.csv")
consensus_labels_path <- paste0(data_base_path, "training_data_4346.csv")

# Load the whole antibiotic prescription dataset, will throw errors due to wrong data types
full_indication <- read_csv("/home/shared/iord_extract_20220325/antibiotic_prescriptions.csv", col_types = cols(
  Clusterid = col_double(),
  PrescriptionID = col_character(),
  PrescriptionDate = col_datetime(format = ""),
  PrescriptionStatus = col_factor(),
  PrescriptionType = col_factor(),
  AdmissionDate = col_date(format = ""),
  Drug = col_factor(),
  Formulation = col_factor(),
  Dose = col_double(),
  DoseUnit = col_character(),
  Frequency = col_character(),
  Route = col_factor(),
  Indication = col_character(),
  ScheduledStartDateTime = col_datetime(format = ""),
  ScheduledStopDateTime = col_datetime(format = ""),
  DurationEnteredByPrescriber = col_character(),
  ReviewDateTime = col_datetime(format = ""),
  FirstAdministrationDateTime = col_datetime(format = ""),
  LastAdministrationDateTime = col_datetime(format = ""),
  NumberOfDosesAdministered = col_double(),
  SpecialInstructions = col_character(),
  AntibioticsSource = col_factor()
)) %>%
  relocate(ClusterID=Clusterid, PrescriptionID, Indication)

# Episode information (contains birth year & month)
patient_birthdate <- read_csv("~/shared/iord_extract_20220325/inpt_episode.csv", col_types = cols(
    ClusterID = col_double(),
    LinkedBirthMonth = col_datetime()
  )) %>%
  distinct(ClusterID, LinkedBirthMonth)


# Hospital Site information
ward_stay <- read_csv("~/shared/iord_extract_20220325/ward_stay.csv", col_types = cols(
  ClusterID = col_double(),
  WardStartDate = col_datetime(format = ""),
  WardEndDate = col_datetime(format = ""),
  WardName = col_factor(),
  Building = col_factor(),
  Facility = col_factor()
))


# Prescriptions with admission (previously joined)
rxid_with_admission <- read_csv(rxid_with_admission_path, show_col_types = FALSE) %>%
  distinct(ClusterID, EpisodeID, PrescriptionID)

# Our existing labels
consensus_labels <- read_csv(consensus_labels_path, show_col_types = FALSE)
```

Obtain & set some general parameters
```{r}
# Columns for the original labelled data
label_columns <- c("urinary", "respiratory", "abdominal", "neurological", "skin_soft_tissue", "ent", "orthopaedic", "other", "no_specific_source", "prophylaxis", "procedural", "immunosuppression", "viral", "uncertainty")

# label_columns <- tail(colnames(consensus_labels), -2)
```

Some general overview of the avaiable wards
```{r}
ward_stay$Facility %>%
  summary
```

Calculate a random seed from a string. Function taken from:
https://rdrr.io/cran/TeachingDemos/src/R/char2seed.R
```{r}
char2seed <- function(x,set=TRUE,...){

	tmp <- c(0:9,0:25,0:25)
	names(tmp) <- c(0:9,letters,LETTERS)

	x <- gsub("[^0-9a-zA-Z]","",as.character(x))

	xsplit <- tmp[ strsplit(x,'')[[1]] ]

	seed <- sum(rev( 7^(seq(along=xsplit)-1) ) * xsplit)
        seed <- as.integer( seed %% (2^31-1) )

	if(set){
		set.seed(seed,...)
		return(invisible(seed))
	} else {
		return(seed)
	}
}

# Obtain a random seed
random_seed_numeric <- char2seed("NLP with Qingze and Chang is really fun!", set=FALSE)
```

## Preprocessing
### Clean the indications
1. Convert to lower case
2. Remove unnecessary whitespaces (trainling and leading whitespaces and double whitespaces)
3. Remove single character indications (except for just ?)
4. Remove words that don't contain letters or question marks (just numbers or just special characters)
```{r}
full_indication_cleaned <- full_indication%>%
  # Convert the indications to lower case
  mutate(Indication=tolower(Indication)) %>%
  # Squish the string (remove unnecessary whitespaces)
  mutate(Indication=str_squish(Indication)) %>%
  # Set minimal length to one (except for ?)
  filter(str_length(Indication) > 1 | Indication == "?") %>%
  # Must contain at least one letter
  filter(str_detect(Indication, "[a-z\\?]"))
```

### Join RxIDs with inpatient episodes
```{r}
indication_w_admission <- full_indication_cleaned %>%
  inner_join(rxid_with_admission, by = c("ClusterID", "PrescriptionID")) %>%
  drop_na(Indication)

indication_wo_admission <- full_indication_cleaned %>%
  anti_join(rxid_with_admission, by = c("ClusterID", "PrescriptionID"))
```

```{r}
indication_w_admission %>%
  nrow()

indication_wo_admission %>%
  nrow()
```

#### Investigate potential linkage error
```{r}
indication_wo_admission %>%
  summary()
```
-> Looks fine. 58 199 cases don't have admission dates, why don't the other ~50k cases link to an admission?

### Filter for patients >= 16 years old
We are only looking into adult cases
```{r}
lower_age_limit <- 16  # In years, inclusive filtering
indication_w_admission_filtered <- indication_w_admission %>%
  inner_join(patient_birthdate, by = join_by(ClusterID)) %>%
  mutate(Age = as.numeric(PrescriptionDate - LinkedBirthMonth, unit="days")/365.25) %>%
  filter(Age >= lower_age_limit)

sprintf("Number of rows removed: %i", nrow(indication_w_admission) - nrow(indication_w_admission_filtered))
```
-> The admission data was already filtered for adults at admission time. We filter for adults at prescription time here


## Seperate the sites in the dataset
The database contains multiple sites, separate them

### Add Location to Facilities
Quick look at available locations before creating the mapping table
```{r}
ward_stay$Facility %>%
  summary()
```
-> Ignore empty string

Map the ward stays and remove ones not in the list (namely empty locations)
```{r}
# Mapping table, make sure all the centres appear.
# Locations not in this table will be removed from the output
location_map <- tibble(Facility=c("John Radcliffe", "Churchill", "Horton", "OUH", "NOC"), 
                       Location=factor(c("Oxford", "Oxford", "Banbury", "Oxford", "Oxford"))
                       )

# Table to use for joins
cluster_location_tbl <- ward_stay %>%
  inner_join(location_map, by = "Facility") %>%
  select(ClusterID, WardStartDate, WardEndDate, Facility, Location)

cluster_location_tbl %>%
  count(Facility, Location, sort = TRUE)
```

### Adaptive Inclusion Window Matching
Allow for increasing inclusion window sizes, let the inclusion time (fuzzy date)
expand but choose the smallest time difference.

Join the indication data with the ward stays. 
This will result in a many-to-many join, and needs to be filtered.
Add `StartDiffDays` and `EndDiffDays` to indicate the location of the prescription time
relative to the ward stay window.
```{r}
indication_w_admission_ward <- indication_w_admission_filtered %>%
  # Join the indications, this should capture almost all of the indications
  left_join(cluster_location_tbl, by = join_by(ClusterID), relationship = "many-to-many") %>%
  # Calculate location of the admission within the ward window
  # mutate(StartDiffDays=difftime(PrescriptionDate, WardStartDate, units="days"),
  # EndDiffDays=difftime(WardEndDate, PrescriptionDate, units="days")
  mutate(StartDiffDays=as.numeric(PrescriptionDate - WardStartDate, units="days"),
  EndDiffDays=as.numeric(WardEndDate - PrescriptionDate, units="days")
         )

tmp_count <- indication_w_admission_ward %>% nrow()

# Remove entries (patients) which didn't have ANY ward stay entries
# We expect only a handful of such cases
indication_w_admission_ward <- indication_w_admission_ward %>%
  filter(!is.na(Facility))

sprintf("Number of unmatched cases removed: %i", tmp_count - nrow(indication_w_admission_ward))
```

Adaptive filtering function
```{r}
# Function to filter for the closest (best) match
adaptive_filtering <- function(input_df, max_fuzzy_day, return_all=FALSE){
  # Filter to this max fuzzyness
  input_df_limited <- input_df %>%
    filter(StartDiffDays >= -max_fuzzy_day, EndDiffDays >= -max_fuzzy_day)

  
  # Match to closest ward stay it is within.
  # 1. Assign "priority". If a duration is negative (outside), it lowers the priority
  # 2. Calculate minimum of absolute distances (start and end)
  # 3. Sort by priority & distance ascending, pick first entry
  input_df_limited_sorted <- input_df_limited %>%
    # Assign priority & get min(abs(), abs()) distance
    mutate(Priority=if_else(StartDiffDays<0, 1, 0) + if_else(EndDiffDays<0, 1, 0),
           TimeDiff=pmin(abs(StartDiffDays), abs(EndDiffDays))
           ) %>%
    # Group by prescription & then sort by priority and difference
    group_by(ClusterID, PrescriptionID) %>%
    arrange(Priority, TimeDiff, .by_group = TRUE)
  
  # Return all the entries, just limited and sorted
  if(return_all){
    return(input_df_limited_sorted)
  }
  
  # Pick the first row per group
  input_df_limited_sorted %>%
    slice(1) %>%
    ungroup() %>%
    select(-StartDiffDays, -EndDiffDays, -Priority, -TimeDiff)
}
```


Plot the time difference and number of missing cases vs error rate
```{r}
min_fuzzy_day <- 0
max_fuzzy_day <- 15

# Prefilter data to remove the bulk of entries (speedup)
tmp_indication_ward_data <- indication_w_admission_ward %>%
  filter(StartDiffDays >= -max_fuzzy_day, EndDiffDays >= -max_fuzzy_day)

# Results tibble
fuzzy_investigation_tbl <- tibble(
  fuzzy_day = numeric(),
  num_matches = numeric(),
  num_error = numeric(),
)

# Iterate through the days
for (fuzzy_day in c(min_fuzzy_day:max_fuzzy_day)) {
  # Print some information
  print(sprintf("Calculating for `fuzzy_date`: %i", fuzzy_day))
  
  # Run the limiting and prioritising function
  tmp_indication_ward_data_limited_sorted <- adaptive_filtering(tmp_indication_ward_data, fuzzy_day, return_all = TRUE)
  
  # Get the number of matches and errors and write back to the table
  num_matches = tmp_indication_ward_data_limited_sorted %>%
    distinct(ClusterID, PrescriptionID) %>%
    nrow()

  num_error = tmp_indication_ward_data_limited_sorted %>%
    filter(TimeDiff <= first(TimeDiff) + 0.04) %>%  # Get the ones that are less than an hour apart
    filter(length(unique(Location)) > 1) %>%
    distinct(ClusterID, PrescriptionID) %>%
    nrow()
  
  # Write to results data frame
  fuzzy_investigation_tbl <- fuzzy_investigation_tbl %>%
    add_row(fuzzy_day=fuzzy_day,
            num_matches=num_matches,
            num_error=num_error)
}

fuzzy_investigation_tbl
```

```{r}
fuzzy_investigation_tbl %>%
  pivot_longer(cols = c(num_matches, num_error)) %>%
  ggplot(aes(x=fuzzy_day, y=(value), colour=name)) + 
    geom_line() +
    xlab("Fuzzy Day") +
    ylab("Count")
```

Print the table
```{r}
fuzzy_investigation_tbl
```

#### Apply to dataset for further processing
```{r}
indication_w_admission_site <- adaptive_filtering(indication_w_admission_ward, max_fuzzy_day = 2)
```

##### [Results] Number of (unique) Indications
Print data set statistics (number of samples per site, number of unique indications)
```{r}
indication_oxford <- indication_w_admission_site %>%
  filter(Location=="Oxford")

indication_banbury <- indication_w_admission_site %>%
  filter(Location=="Banbury")

sprintf("Oxford indications: %i (total), %i (unique)", nrow(indication_oxford), n_distinct(indication_oxford$Indication))

sprintf("Banbury indications: %i (total), %i (unique)", nrow(indication_banbury), n_distinct(indication_banbury$Indication))
```

## Evaluate Training Data Set Size
How large must the training data set be to cover say 75% of all indications?
Start with labelling the most common ones.

Calculate the total coverage of indication by label size
```{r}
# Calculate coverage by label size
labelled_size_vs_total_coverage <- indication_oxford %>%
  # Count occurrence of indications
  count(Indication, sort=TRUE) %>%
  # Calculate cumulative sum and save row_number (label count)
  mutate(total_coverage=cumsum(n)/sum(n) * 100, label_count=row_number())
```

### [Results] Plot: Coverage vs Labelled Data Set Size
```{r}
ggplot(labelled_size_vs_total_coverage, aes(x=label_count, y=total_coverage)) +
  # Plot data
  geom_line() +
  
  # Plot vertical line
  geom_vline(xintercept=4000, linetype="dashed", colour = "blue") +
  
  # Transform X-Axis
  scale_x_continuous(
    breaks = c(1, 10, 100, 1000, 4000, 10000),
    trans="log10",
  ) +
  annotation_logticks(sides="b") +

  # Set Y-Axis limits
  ylim(0, 100) +
  
  # Labels and legends
  labs(
    title = "Labelled Data Set Size vs Total Indication Coverage",
    x = "Labelled Set Size, log10 scale [count]", 
    y = "Total Indication Coverage [%]",
  ) +
  # Styling
  theme_light()
    
```
### [Results] Labelled data (4000) vs total coverage
Give the coverage the top 4000 labelled indications will have
```{r}
labelled_size_vs_total_coverage %>%
  filter(label_count %in% c(3500, 4000))
```

### [Results] Top 10 Indications vs total coverage
```{r}
labelled_size_vs_total_coverage %>%
  mutate(relative_coverage=round(proportions(n)*100, 2))
```



## Oxford
### Split the data into train-test sets
```{r}
# Set train/test split ratio
train_ratio = 0.9
test_ratio = 1 - train_ratio

# Calculate training mask
set.seed(random_seed_numeric)
train_ind <- sample.int(nrow(indication_oxford), size = floor(nrow(indication_oxford)) * train_ratio, replace = FALSE)

oxford_train_full <- indication_oxford[train_ind, ]
oxford_test_full <- indication_oxford[-train_ind, ]
```

### Sample from train & test set
Get the 4000 training samples (4k most common indications)
```{r}
oxford_train_4000 <- oxford_train_full %>%
    count(Indication, name = "Count", sort=TRUE)  %>%
    head(4000)
```

Randomly sample for the 2000 test samples
```{r}
test_set_size = 2000

set.seed(random_seed_numeric)
oxford_test_2000 <- oxford_test_full %>%
   slice_sample(n=test_set_size, replace=FALSE)
```


### Evaluate re-labling needs
How many of the indications still need to be labelled?

Training Set:
```{r}
oxford_train_4000_missing <- oxford_train_4000 %>%
    anti_join(consensus_labels, by = join_by(Indication))

# Add empty columns
oxford_train_4000_missing[, label_columns] <- NA

sprintf("Missing training indications: %i", nrow(oxford_train_4000_missing))
```

Test Set:
```{r}
oxford_test_2000_missing <- oxford_test_2000 %>%
    anti_join(consensus_labels, by = join_by(Indication)) %>%
    count(Indication, name = "Count", sort=TRUE)

# Add empty columns
oxford_test_2000_missing[, label_columns] <- NA

sprintf("Missing test indications: %i", nrow(oxford_test_2000_missing))
```


### Export training data set
Write the whole Oxford data
```{r}
indication_oxford %>%
  write_csv(paste0(output_path, "/Oxford/Full_Oxford_Data_Set.csv"))
```

Write the whole Banbury data
```{r}
indication_banbury %>%
  write_csv(paste0(output_path, "/Banbury/Full_Banbury_Data_Set.csv"))
```
**Training Data**
The whole traiing data
```{r}
oxford_train_full %>% 
  write_csv(paste0(output_path, "/Oxford/Train_Set_Full.csv"))
```

Top 4000 of the training data
```{r}
oxford_train_4000 %>% 
  write_csv(paste0(output_path, "/Oxford/Train_Set_4000.csv"))
```

Write the missing indications for the top 4000 ones to disk
```{r}
oxford_train_4000_missing %>% 
  write_csv(paste0(output_path, "/Oxford/Train_Set_4000_Missing.csv"))
```

**Test Data**
Full Test Data
```{r}
oxford_test_full %>% 
  write_csv(paste0(output_path, "/Oxford/Test_Set_Full.csv"))
```

Top 2000 of the test data
```{r}
oxford_test_2000 %>% 
  write_csv(paste0(output_path, "/Oxford/Test_Set_2000.csv"))
```

Write the missing indications for the top 4000 ones to disk
```{r}
oxford_test_2000_missing %>% 
  write_csv(paste0(output_path, "/Oxford/Test_Set_2000_Missing.csv"))
```

## Banbury
-> All Test Set
### Test Set Preparations (evaluate size & write files)
How many samples need to be labeled additionally for a test set of size xx?
Figure out and write all sizes to disk for further labelling.

```{r}
# Define test set sizes
test_set_sizes <- c(500, 1000, 2000, 4000, 5000, 8000, 10000)

info_lines <- c()

# Existing Labels
existing_labels <- tibble(Indication = unique(
    c(consensus_labels$Indication, oxford_train_4000$Indication)
  ))

for (test_set_size in test_set_sizes) {
  # Sample a subset
  set.seed(random_seed_numeric)
  sub_sample <- indication_banbury %>%
    slice_sample(n=test_set_size, replace=FALSE)

  # Match with existing labels & count missing
  sub_sample_unmatched <- sub_sample %>%
    anti_join(existing_labels, by = join_by("Indication"))

  # Obtain a count
  n_missing_labels <- sub_sample_unmatched %>%
    distinct(Indication) %>%
    nrow()

  # Write the file(s)
  sub_sample_unmatched_writing <- sub_sample_unmatched %>%
    distinct(Indication)

  sub_sample_unmatched_writing[, label_columns] <- NA

  sub_sample_unmatched_writing %>%
    write_csv(sprintf("%s/Banbury/Test_Set_%i_Missing.csv", output_path, test_set_size))

  sub_sample %>%
    write_csv(sprintf("%s/Banbury/Test_Set_%i_Full.csv", output_path, test_set_size))

  # Print & Write information
  file_information_string <- sprintf("Test set size: %i - %i missing labels", test_set_size, n_missing_labels )
  print(file_information_string)
  info_lines <- append(info_lines, file_information_string)


}

# Open a file to write comments to

fileConn<-file(paste0(output_path, "/Banbury/Info.txt"))
writeLines(info_lines, fileConn)
close(fileConn)
```