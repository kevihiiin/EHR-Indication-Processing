{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark: Training\n",
    "\n",
    "How long it takes to train the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "- Import libraries\n",
    "- Import datasets\n",
    "- Run some basic preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kevinyuan/EHR-Indication-Processing/02_Models/04_Benchmarks\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from functools import wraps\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score, average_precision_score, classification_report\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters & paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paths\n",
    "# Base data path\n",
    "base_data_path = Path(\"../../00_Data/\")\n",
    "# Data Path (training, testing, etc.)\n",
    "data_path =  base_data_path / \"publication_ready\"\n",
    "\n",
    "\n",
    "assert base_data_path.is_dir(),\\\n",
    "  f\"{base_data_path} either doesn't exist or is not a directory.\"\n",
    "\n",
    "# --- Bechmark parameters\n",
    "# How often to repeat the training and testing\n",
    "n_repeat = 10\n",
    "#  Testset size\n",
    "n_test_sizes = [10, 100, 1000, 10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set size overview:\n",
      "- Training set: 3400\n",
      "- Evaluation set: 600\n",
      "- Testing Oxford set: 2000\n",
      "- Testing Banbury set: 2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import data --> upload into \"Files\" on the left-hand panel\n",
    "train_eval_df = pd.read_csv(\n",
    "    data_path / 'training_oxford_2023-08-23.csv',\n",
    "    dtype={\"Indication\": str},\n",
    "    keep_default_na=False,\n",
    "    na_values=[\"NA\"],\n",
    ")\n",
    "\n",
    "test_oxford_df = pd.read_csv(\n",
    "    data_path / 'testing_oxford_2023-08-23.csv',\n",
    "    dtype={\"Indication\": str},\n",
    "    keep_default_na=False,\n",
    "    na_values=[\"NA\"],\n",
    ")\n",
    "\n",
    "test_banbury_df = pd.read_csv(\n",
    "    data_path / 'testing_banbury_2023-08-23.csv',\n",
    "    dtype={\"Indication\": str},\n",
    "    keep_default_na=False,\n",
    "    na_values=[\"NA\"],\n",
    ")\n",
    "\n",
    "# --- Split into train and eval\n",
    "train_df, eval_df = train_test_split(\n",
    "    train_eval_df, \n",
    "    test_size=0.15,\n",
    "    random_state=42,\n",
    "    shuffle=True)\n",
    "\n",
    "print(\"Data set size overview:\")\n",
    "print(f\"- Training set: {train_df.shape[0]}\")\n",
    "print(f\"- Evaluation set: {eval_df.shape[0]}\")\n",
    "print(f\"- Testing Oxford set: {test_oxford_df.shape[0]}\")\n",
    "print(f\"- Testing Banbury set: {test_banbury_df.shape[0]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert labels to numbers and get prettier labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Urinary',\n",
       " 'Respiratory',\n",
       " 'Abdominal',\n",
       " 'Neurological',\n",
       " 'Skin Soft Tissue',\n",
       " 'Ent',\n",
       " 'Orthopaedic',\n",
       " 'Other Specific',\n",
       " 'No Specific Source',\n",
       " 'Prophylaxis',\n",
       " 'Uncertainty',\n",
       " 'Not Informative']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels\n",
    "labels = [label for label in train_df.columns if label not in [\"Indication\"]]\n",
    "labels_pretty = [\" \".join(word.capitalize() for word in label.split(\"_\")) for label in labels]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "labels2labels_pretty = {old:pretty for old, pretty in zip(labels, labels_pretty)}\n",
    "\n",
    "labels_pretty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_data = pd.concat([test_oxford_df.Indication, test_banbury_df.Indication]).repeat(5)\n",
    "\n",
    "inference_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bechmark functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeit(func):\n",
    "    @wraps(func)\n",
    "    def timeit_wrapper(*args, **kwargs):\n",
    "        duration_list = []\n",
    "        for i in range(0, n_repeat):\n",
    "            print(\"Iteration:\", i+1)\n",
    "\n",
    "            # Start timer & process\n",
    "            start_time = time.process_time()\n",
    "            result = func(*args, **kwargs)\n",
    "            end_time = time.process_time()\n",
    "            process_duration = end_time - start_time\n",
    "            \n",
    "            # Save to list and report\n",
    "            print(f'Took {process_duration:.2f} seconds')\n",
    "            duration_list.append(process_duration)\n",
    "\n",
    "        # Calculate the average sample time\n",
    "        duration_mean = pd.Series(duration_list).mean()\n",
    "        print(f\"The average execution time for 10k samples took {(duration_mean/len(inference_data)*10000):.2f}s\")\n",
    "\n",
    "        return result, duration_mean\n",
    "    return timeit_wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import regex\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Regex Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['urinary', 'respiratory', 'abdominal', 'neurological', 'skin_soft_tissue', 'ent', 'orthopaedic', 'other_specific', 'no_specific_source', 'prophylaxis'])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "debug = True\n",
    "\n",
    "model_path = base_data_path / \"Regex\"\n",
    "\n",
    "rule_sheet_annotated_path = model_path / \"regex_rules_sheet_annotated.xlsx\"\n",
    "regex_rule_sheets = pd.read_excel(rule_sheet_annotated_path, sheet_name=None, index_col=None)\n",
    "\n",
    "categories = ['urinary', 'respiratory', 'abdominal',\n",
    "       'neurological', 'skin_soft_tissue', 'ent', 'orthopaedic',\n",
    "       'other_specific', 'no_specific_source', 'prophylaxis']\n",
    "\n",
    "if debug:\n",
    "    display(regex_rule_sheets.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some baseline parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global parameters\n",
    "default_error = 0.1 # percent error allowed (0.1 = 10%)\n",
    "default_error_max = 2 # max number of errors allowed\n",
    "default_l_boundary = r\"\\b\"\n",
    "default_r_boundary = r\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the rules to regex strings and the compile into patterns (allows for faster matching).\n",
    "\n",
    "Create a dictionary with patterns for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abdominal': regex.Regex('(?:\\\\bcholecystitis){e<=2}|(?:\\\\bdiverticulitis){e<=2}|(?:\\\\bappendicitis){e<=2}|(?:\\\\bpid\\\\b){e<=0}|(?:\\\\bbiliary sepsis){e<=2}|(?:\\\\bcholangitis){e<=2}|(?:\\\\bh pylori eradication){e<=1}|(?:\\\\babdo sepsis){e<=2}|(?:\\\\bperianal abscess){e<=2}|(?:\\\\babdominal sepsis){e<=2}|(?:\\\\bintra-abdominal sepsis){e<=2}|(?:\\\\bcolitis){e<=1}|(?:\\\\bsplenectomy){e<=2}|(?:\\\\bc diff){e<=1}|(?:\\\\bsuspected c. diff){e<=2}|(?:\\\\bintra-abdominal infection){e<=2}|(?:\\\\bliver abscess){e<=2}|(?:\\\\bpd peritonitis){e<=2}|(?:\\\\bintraabdominal sepsis){e<=2}|(?:\\\\bh pylori){e<=0}|(?:\\\\bsbp\\\\b){e<=0}|(?:\\\\bintrabdominal sepsis){e<=2}|(?:\\\\bpelvic collection){e<=2}|(?:\\\\babdominal collection){e<=2}|(?:\\\\bintra-abdo sepsis){e<=2}|(?:\\\\babdominal infection){e<=2}|(?:\\\\bc.diff){e<=1}|(?:\\\\bintra abdo sepsis){e<=2}', flags=regex.V0),\n",
      " 'ent': regex.Regex('(?:\\\\btonsillitis){e<=2}|(?:\\\\bquinsy){e<=1}|(?:\\\\btonsilitis){e<=1}|(?:\\\\bsinusitis){e<=1}|(?:\\\\bsupraglottitis){e<=2}|(?:\\\\bpinna cellulitis){e<=2}|(?:\\\\bperichondritis){e<=2}|(?:\\\\bpost tonsillectomy bleed){e<=2}|(?:\\\\bnoe\\\\b){e<=0}|(?:\\\\bneck abscess){e<=2}|(?:\\\\bpharyngitis){e<=2}|(?:\\\\bparapharyngeal abscess){e<=2}|(?:\\\\bpost-tonsillectomy bleed){e<=2}|(?:\\\\botitis media){e<=2}|(?:\\\\bepiglottitis){e<=2}|(?:\\\\botitis externa){e<=2}|(?:\\\\bmastoiditis){e<=2}|(?:\\\\bsevere oe\\\\b){e<=1}|(?:\\\\bperitonsillar abscess){e<=2}|(?:\\\\bmoe\\\\b){e<=0}|(?:\\\\bear infection){e<=2}|(?:\\\\bneck infection){e<=2}|(?:\\\\bsialadenitis){e<=2}|(?:\\\\bpost tonsillectomy){e<=2}|(?:\\\\bmalignant otitis externa){e<=2}|(?:\\\\bpost cochlear implant){e<=2}|(?:\\\\bnasal packing){e<=2}|(?:\\\\bepistaxis){e<=1}|(?:\\\\bnasal pack){e<=1}', flags=regex.V0),\n",
      " 'neurological': regex.Regex('(?:\\\\bcns infection\\\\b){e<=0}|(?:\\\\bmeningitis){e<=1}|(?:\\\\bencephalitis){e<=2}|(?:\\\\bbrain abscess){e<=2}|(?:\\\\bcerebral abscess){e<=2}|(?:\\\\bventriculitis){e<=2}|(?:\\\\bcsf infection){e<=0}|(?:\\\\bsubdural empyema){e<=2}|(?:\\\\bempiric cns neurosurg){e<=2}|(?:\\\\bmeningoencephalitis){e<=2}|(?:\\\\binfected cranioplasty){e<=2}|(?:\\\\bintracranial infection){e<=2}|(?:\\\\bepidural abscess){e<=2}|(?:\\\\bbone flap infection){e<=2}|(?:\\\\bshunt infection){e<=2}|(?:\\\\bcns sepsis){e<=0}|(?:\\\\bcranioplasty infection){e<=2}|(?:\\\\bcsf leak){e<=0}|(?:\\\\bskull base osteomyelitis){e<=2}|(?:\\\\bcerebral absces empirical){e<=2}|(?:\\\\bcerebritis){e<=1}|(?:\\\\bloading dose empiric cns\\\\b){e<=0}|(?:\\\\bscs\\\\b){e<=0}', flags=regex.V0),\n",
      " 'no_specific_source': regex.Regex('(?:\\\\bperioperative prophylaxis){e<=2}|(?:\\\\bsepsis){e<=1}|(?:\\\\bprophylaxis){e<=2}|(?:\\\\binfection){e<=1}|(?:\\\\bpost op){e<=1}|(?:\\\\bpost-op){e<=1}|(?:\\\\babscess){e<=1}|(?:\\\\bneutropenic fever){e<=2}|(?:\\\\bcovid-19){e<=0}|(?:\\\\bfever){e<=1}|(?:\\\\bpostop){e<=1}|(?:\\\\bpyrexia in labour){e<=2}|(?:\\\\btb\\\\b){e<=0}|(?:\\\\bpsa\\\\b){e<=0}|(?:\\\\bsurgery){e<=1}', flags=regex.V0),\n",
      " 'orthopaedic': regex.Regex('(?:\\\\bosteomyelitis){e<=2}|(?:\\\\bpji\\\\b){e<=0}|(?:\\\\bbone infection){e<=2}|(?:\\\\bom\\\\b){e<=0}|(?:\\\\bopen fracture){e<=2}|(?:\\\\bseptic arthritis){e<=2}|(?:\\\\bjoint infection){e<=2}|(?:\\\\bdiscitis){e<=1}|(?:\\\\bbone inf){e<=1}|(?:\\\\binfected tkr\\\\b){e<=1}|(?:\\\\binfected metalwork){e<=2}|(?:\\\\bspinal infection){e<=2}|(?:\\\\binfected knee){e<=2}|(?:\\\\binfected thr\\\\b){e<=1}|(?:\\\\bknee infection){e<=2}|(?:\\\\binfected hip){e<=2}|(?:\\\\bdair\\\\b){e<=0}|(?:\\\\bpsoas abscess){e<=2}|(?:\\\\bmetalwork infection){e<=2}|(?:\\\\bseptic joint){e<=2}|(?:\\\\bhip infection){e<=0}|(?:\\\\binfected metal work){e<=2}|(?:\\\\bolecranon bursitis){e<=2}|(?:\\\\bseptic knee){e<=2}|(?:\\\\bspinal wound infection){e<=2}|(?:\\\\binfected joint){e<=2}|(?:\\\\bosteomylitis){e<=2}|(?:\\\\bspinal abscess){e<=2}|(?:\\\\bopen skull fracture){e<=2}|(?:\\\\bprosthetic joint infectio){e<=2}', flags=regex.V0),\n",
      " 'other_specific': regex.Regex('(?:\\\\bdental abscess){e<=2}|(?:\\\\boral thrush){e<=2}|(?:\\\\bendometritis){e<=2}|(?:\\\\bchorioamnionitis){e<=2}|(?:\\\\bpprom\\\\b){e<=0}|(?:\\\\bdental infection){e<=2}|(?:\\\\bie\\\\b){e<=0}|(?:\\\\bpre ppm\\\\b){e<=0}|(?:\\\\bppm\\\\b){e<=0}|(?:\\\\bendocarditis){e<=2}|(?:\\\\boral candida){e<=2}|(?:\\\\boral candidiasis){e<=2}|(?:\\\\bmrop\\\\b){e<=0}|(?:\\\\bline infection){e<=2}|(?:\\\\bprom\\\\b){e<=0}|(?:\\\\bparotitis){e<=1}|(?:\\\\bdental abcess){e<=2}|(?:\\\\brpoc\\\\b){e<=0}|(?:\\\\bline sepsis){e<=2}|(?:\\\\bmandible fracture){e<=2}|(?:\\\\bbox change){e<=1}|(?:\\\\bstent removal){e<=2}|(?:\\\\binstrumental delivery){e<=2}|(?:\\\\bpre-ppm\\\\b){e<=0}|(?:\\\\bfractured mandible){e<=2}|(?:\\\\bvaginal pack){e<=2}|(?:\\\\bchorio){e<=0}|(?:\\\\bpsrom\\\\b){e<=0}', flags=regex.V0),\n",
      " 'prophylaxis': regex.Regex('(?:\\\\bperioperative prophylaxis){e<=2}|(?:\\\\bprophylaxis){e<=2}|(?:\\\\bpost op\\\\b){e<=0}|(?:\\\\bpost-op\\\\b){e<=0}|(?:\\\\bpostop\\\\b){e<=0}|(?:\\\\bcauti\\\\b){e<=0}|(?:\\\\bpost op infection){e<=2}|(?:\\\\bpprom\\\\b){e<=0}|(?:\\\\bsplenectomy){e<=2}|(?:\\\\bsurgery){e<=1}|(?:\\\\bprophylactic){e<=2}|(?:\\\\bpre ppm\\\\b){e<=0}|(?:\\\\bppm\\\\b){e<=0}|(?:\\\\bpre-op\\\\b){e<=0}|(?:\\\\bpre procedure){e<=2}|(?:\\\\bpre-procedure){e<=2}|(?:\\\\bpreop\\\\b){e<=0}', flags=regex.V0),\n",
      " 'respiratory': regex.Regex('(?:\\\\blrti\\\\b){e<=0}|(?:\\\\bcap\\\\b){e<=0}|(?:\\\\bhap\\\\b){e<=0}|(?:\\\\bpneumonia){e<=1}|(?:\\\\biecopd\\\\b){e<=0}|(?:\\\\bchest sepsis){e<=2}|(?:\\\\bpcp prophylaxis){e<=0}|(?:\\\\bempyema){e<=1}|(?:\\\\bcf\\\\b){e<=0}|(?:\\\\bchest){e<=1}|(?:\\\\bbronchiectasis){e<=2}|(?:\\\\baspiration){e<=1}|(?:\\\\bie copd\\\\b){e<=0}|(?:\\\\bp. jirovecci prophylaxis){e<=1}|(?:\\\\brti\\\\b){e<=0}|(?:\\\\bie asthma){e<=0}|(?:\\\\biecf\\\\b){e<=0}|(?:\\\\bcopd\\\\b){e<=0}|(?:\\\\bpcp\\\\b){e<=0}|(?:\\\\bpleural infection){e<=2}|(?:\\\\burti\\\\b){e<=0}|(?:\\\\bcystic fibrosis){e<=2}', flags=regex.V0),\n",
      " 'skin_soft_tissue': regex.Regex('(?:\\\\bcellulitis){e<=1}|(?:\\\\bwound infection){e<=2}|(?:\\\\bdog bite){e<=1}|(?:\\\\bdiabetic foot infection){e<=2}|(?:\\\\b3rd degree tear){e<=2}|(?:\\\\bfoot infection){e<=2}|(?:\\\\bcat bite){e<=1}|(?:\\\\bbreast abscess){e<=2}|(?:\\\\bmastitis){e<=1}|(?:\\\\bsoft tissue infection){e<=2}|(?:\\\\bshingles){e<=1}|(?:\\\\bthrush){e<=1}|(?:\\\\bdiabetic foot){e<=2}|(?:\\\\bpilonidal abscess){e<=2}|(?:\\\\bgroin abscess){e<=2}|(?:\\\\binfected wound){e<=2}|(?:\\\\bperichondritis){e<=2}|(?:\\\\bskin infection){e<=2}|(?:\\\\bflexor sheath infection){e<=2}|(?:\\\\bsternal wound infection){e<=2}|(?:\\\\bwound){e<=1}|(?:\\\\bhand infection){e<=2}|(?:\\\\b3b tear){e<=1}|(?:\\\\b3a tear){e<=1}|(?:\\\\binfected leg ulcers){e<=2}|(?:\\\\binfected foot){e<=2}', flags=regex.V0),\n",
      " 'urinary': regex.Regex('(?:\\\\buti\\\\b){e<=0}|(?:\\\\burosepsis){e<=1}|(?:\\\\bpyelonephritis){e<=2}|(?:\\\\bcauti\\\\b){e<=0}|(?:\\\\bepididymo-orchitis){e<=2}|(?:\\\\burinary tract infection){e<=2}|(?:\\\\burinary sepsis){e<=2}|(?:\\\\bepididymitis){e<=2}|(?:\\\\bprostatitis){e<=2}|(?:\\\\bcystoscopy){e<=1}|(?:\\\\burine infection){e<=2}|(?:\\\\bfor latp biopsy){e<=2}|(?:\\\\brenal transplant){e<=2}|(?:\\\\bepididymoorchitis){e<=2}|(?:\\\\borchitis){e<=1}|(?:\\\\burine){e<=1}', flags=regex.V0)}\n"
     ]
    }
   ],
   "source": [
    "regex_pattern_dict = dict()\n",
    "\n",
    "for individual_category in regex_rule_sheets.keys():\n",
    "    # Extract an individual sheet\n",
    "    rule_sheet_individual = regex_rule_sheets[individual_category]\n",
    "\n",
    "    regex_pattern_list = []\n",
    "    for _, row in rule_sheet_individual.iterrows():\n",
    "        # Skip the row if it is marked to be excluded\n",
    "        if row['Exclude'] == 1:\n",
    "            continue\n",
    "        \n",
    "        # Populate with individual regex patterns\n",
    "        str_indication = row['Indication']\n",
    "        num_error = (default_error * len(str_indication)) if pd.isna(row['Error']) else row['Error']\n",
    "        num_error = min(math.ceil(num_error), default_error_max)\n",
    "\n",
    "        pat_l_boundary = default_l_boundary if pd.isna(row['L_Boundary']) else r'\\b'\n",
    "        pat_r_boundary = default_r_boundary if pd.isna(row['R_Boundary']) else r'\\b'\n",
    "\n",
    "        regex_pattern_list += [fr'(?:{pat_l_boundary}{str_indication}{pat_r_boundary}){{e<={num_error}}}']\n",
    "\n",
    "    regex_pattern_dict[individual_category] = regex.compile(\"|\".join(regex_pattern_list))\n",
    "\n",
    "if debug:\n",
    "    pprint(regex_pattern_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "regex.Regex('\\\\?|/|suspected|possible|probable|likely', flags=regex.V0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncertainty_pattern = regex.compile(r\"\\?|/|suspected|possible|probable|likely\")\n",
    "uncertainty_pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferrence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data and run first uncertainty rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to extract the matched pattern\n",
    "def match_pattern(x, regex_pattern):\n",
    "    if match_obj := regex_pattern.search(x):\n",
    "        return match_obj.group()\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_2 = ['urinary',\n",
    " 'respiratory',\n",
    " 'abdominal',\n",
    " 'neurological',\n",
    " 'skin_soft_tissue',\n",
    " 'ent',\n",
    " 'orthopaedic',\n",
    " 'other_specific',\n",
    " 'prophylaxis']\n",
    "\n",
    "@timeit\n",
    "def run_regex(prediction_df):\n",
    "        # Run uncertainty pattern check\n",
    "        prediction_df[\"uncertainty\"] = prediction_df.Indication.apply(lambda x: match_pattern(x, uncertainty_pattern))\n",
    "\n",
    "        # Split words and remove uncertainty markers\n",
    "        prediction_df[\"Indication\"] = (prediction_df[\"Indication\"]\n",
    "                .apply(lambda x: x.replace('?', '').strip()) # Remove ? from all cells\n",
    "                .apply(lambda x: x.replace('/', ' ').strip()) # Split words by \"/\"\n",
    "        )\n",
    "\n",
    "        # Run the regex rules on the columns\n",
    "        for single_category in categories:\n",
    "                # Get the pattern for the current category\n",
    "                regex_pattern = regex_pattern_dict[single_category]\n",
    "                # Applyt the regex and save back\n",
    "                prediction_df[single_category] = prediction_df.Indication.apply(lambda x: match_pattern(x, regex_pattern))\n",
    "\n",
    "        # Reorder columns\n",
    "        prediction_df = prediction_df[['Indication'] + categories + ['uncertainty']]\n",
    "\n",
    "        # Apply last `uncertainty` and `not informative` rule\n",
    "        # Change value of \"uncertainty\" column to \"multiple entries\" if there are multiple entries\n",
    "        prediction_df.loc[prediction_df[categories_2].notna().sum(axis=1) > 1, \"uncertainty\"] = \"multiple entries\"\n",
    "\n",
    "\n",
    "        # Not informative indicator\n",
    "        prediction_df[\"not_informative\"] = prediction_df[categories].apply(\n",
    "                lambda x: True if sum(~x.isna()) == 0 else None,  # If no entries, then True\n",
    "                axis=1\n",
    "        )\n",
    "\n",
    "        return prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_657399/967672845.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prediction_df[\"not_informative\"] = prediction_df[categories].apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 12.89 seconds\n",
      "Iteration: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_657399/967672845.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prediction_df[\"not_informative\"] = prediction_df[categories].apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 12.83 seconds\n",
      "Iteration: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_657399/967672845.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prediction_df[\"not_informative\"] = prediction_df[categories].apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 12.83 seconds\n",
      "Iteration: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_657399/967672845.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prediction_df[\"not_informative\"] = prediction_df[categories].apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 12.82 seconds\n",
      "Iteration: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_657399/967672845.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prediction_df[\"not_informative\"] = prediction_df[categories].apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 12.82 seconds\n",
      "Iteration: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_657399/967672845.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prediction_df[\"not_informative\"] = prediction_df[categories].apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 12.68 seconds\n",
      "Iteration: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_657399/967672845.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prediction_df[\"not_informative\"] = prediction_df[categories].apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 12.82 seconds\n",
      "Iteration: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_657399/967672845.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prediction_df[\"not_informative\"] = prediction_df[categories].apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 12.82 seconds\n",
      "Iteration: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_657399/967672845.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prediction_df[\"not_informative\"] = prediction_df[categories].apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 12.82 seconds\n",
      "Iteration: 10\n",
      "Took 12.81 seconds\n",
      "The average execution time for 10k samples took 6.41s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_657399/967672845.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prediction_df[\"not_informative\"] = prediction_df[categories].apply(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(     Indication urinary respiratory abdominal neurological skin_soft_tissue  \\\n",
       " 0      h.pylori    None        None      None         None             None   \n",
       " 0      h.pylori    None        None      None         None             None   \n",
       " 0      h.pylori    None        None      None         None             None   \n",
       " 0      h.pylori    None        None      None         None             None   \n",
       " 0      h.pylori    None        None      None         None             None   \n",
       " ...         ...     ...         ...       ...          ...              ...   \n",
       " 1999        cap    None         cap      None         None             None   \n",
       " 1999        cap    None         cap      None         None             None   \n",
       " 1999        cap    None         cap      None         None             None   \n",
       " 1999        cap    None         cap      None         None             None   \n",
       " 1999        cap    None         cap      None         None             None   \n",
       " \n",
       "        ent orthopaedic other_specific no_specific_source prophylaxis  \\\n",
       " 0     None        None           None               None        None   \n",
       " 0     None        None           None               None        None   \n",
       " 0     None        None           None               None        None   \n",
       " 0     None        None           None               None        None   \n",
       " 0     None        None           None               None        None   \n",
       " ...    ...         ...            ...                ...         ...   \n",
       " 1999  None        None           None               None        None   \n",
       " 1999  None        None           None               None        None   \n",
       " 1999  None        None           None               None        None   \n",
       " 1999  None        None           None               None        None   \n",
       " 1999  None        None           None               None        None   \n",
       " \n",
       "      uncertainty not_informative  \n",
       " 0           None            True  \n",
       " 0           None            True  \n",
       " 0           None            True  \n",
       " 0           None            True  \n",
       " 0           None            True  \n",
       " ...          ...             ...  \n",
       " 1999        None            None  \n",
       " 1999        None            None  \n",
       " 1999        None            None  \n",
       " 1999        None            None  \n",
       " 1999        None            None  \n",
       " \n",
       " [20000 rows x 13 columns],\n",
       " 12.8157065892)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_regex(pd.DataFrame(inference_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y train and test\n",
    "X_train, y_train = train_df[\"Indication\"], train_df.drop(\"Indication\", axis=1)\n",
    "\n",
    "# Tokenize the text using n-grams\n",
    "cv = CountVectorizer(ngram_range=(1, 3))\n",
    "X_train = cv.fit_transform(X_train)\n",
    "X_test = cv.transform(test_oxford_df[\"Indication\"])\n",
    "X_inference_xgb = cv.transform(inference_data)\n",
    "\n",
    "y_test = test_oxford_df[labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultiOutputClassifier(xgb.XGBClassifier())\\\n",
    "    .fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick check whether the model is predicting correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.88      0.93       160\n",
      "           1       1.00      0.86      0.92       374\n",
      "           2       0.96      0.67      0.79       239\n",
      "           3       1.00      0.30      0.47        23\n",
      "           4       0.93      0.65      0.77       131\n",
      "           5       0.86      0.12      0.21        51\n",
      "           6       0.89      0.44      0.59        55\n",
      "           7       0.79      0.43      0.56        88\n",
      "           8       0.96      0.83      0.89       906\n",
      "           9       0.99      0.90      0.94       704\n",
      "          10       0.54      0.36      0.43       174\n",
      "          11       1.00      0.82      0.90        34\n",
      "\n",
      "   micro avg       0.95      0.77      0.85      2939\n",
      "   macro avg       0.91      0.61      0.70      2939\n",
      "weighted avg       0.94      0.77      0.84      2939\n",
      " samples avg       0.77      0.75      0.75      2939\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevinyuan/EHR-Indication-Processing/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Took 3.26 seconds\n",
      "Iteration: 2\n",
      "Took 2.85 seconds\n",
      "Iteration: 3\n",
      "Took 2.87 seconds\n",
      "Iteration: 4\n",
      "Took 4.90 seconds\n",
      "Iteration: 5\n",
      "Took 6.14 seconds\n",
      "Iteration: 6\n",
      "Took 2.74 seconds\n",
      "Iteration: 7\n",
      "Took 2.87 seconds\n",
      "Iteration: 8\n",
      "Took 2.58 seconds\n",
      "Iteration: 9\n",
      "Took 3.02 seconds\n",
      "Iteration: 10\n",
      "Took 2.16 seconds\n",
      "The average execution time for 10k samples took 1.67s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 3.3403470788000105)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@timeit\n",
    "def run_xgboost(data):\n",
    "    clf.predict(data)\n",
    "\n",
    "\n",
    "run_xgboost(X_inference_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bio_ClinicalBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Transformers (Huggingface) and PyTorch Imports\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForSequenceClassification\n",
    "from transformers.pipelines.pt_utils import KeyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "model_location_path = base_data_path / \"model_output\"  # Path to the model & data\n",
    "saved_model_name = \"Bio_ClinicalBERT_5615.pth\"  # Saved model name\n",
    "model_hf_id = \"emilyalsentzer/Bio_ClinicalBERT\"  # Currently required for the correct tokenizer\n",
    "cuda_device = \"cpu\"  # Change to \"0\" for the first GPU, or \"cpu\" for CPU\n",
    "\n",
    "batch_size = 8  # Batch size for inference, we used 8 for the training of this model, keep it at 8\n",
    "pred_threshold = 0.5  # Threshold for binarising the predictions, we choose 0.5 for training, can be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "106b60601ce04ed78fc5ff2ee13b65f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 3384.22 seconds\n",
      "Iteration: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f37e6df6898944519faaad84237549fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 3320.49 seconds\n",
      "Iteration: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f28ac1dd30244f89be03f650b451f1ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 3351.15 seconds\n",
      "Iteration: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18dc6fca57594a92a43dc294266c0fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 3694.18 seconds\n",
      "Iteration: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e76c18c610744f4a2175be10f08a15a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 3551.72 seconds\n",
      "Iteration: 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bdaf00ea14d44669b0dadeb3d694a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 3512.71 seconds\n",
      "Iteration: 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "330c0eb8dce74b7a976c1e333731ea7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 3376.81 seconds\n",
      "Iteration: 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34f26d669554fb6888253d081fae6db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 3785.58 seconds\n",
      "Iteration: 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a765ba9d7ad45a8b60cd4af5469dd56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 3511.79 seconds\n",
      "Iteration: 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b549be89044e6e8ca392785f9e5912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 3437.05 seconds\n",
      "The average execution time for 10k samples took 1746.28s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 3492.5691460724)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Preprocess Data ---\n",
    "# Convert to lower case and make unique\n",
    "inference_df = pd.DataFrame({\"Input_String\": inference_data})\n",
    "# Create a Huggingface Dataset\n",
    "inference_dataset = Dataset.from_pandas(inference_df)\n",
    "\n",
    "# --- Loading Model & Inference ---\n",
    "@timeit\n",
    "def bert_inference(input_data):\n",
    "    # Load the model and tokeniser\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_location_path / saved_model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_hf_id)\n",
    "    # Set up the pipeline\n",
    "    inference_pipeline = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, top_k=None, device=cuda_device)\n",
    "\n",
    "    # Classify the infections, runs in batches\n",
    "    pred_output = []\n",
    "    for out in tqdm(\n",
    "            inference_pipeline(\n",
    "                KeyDataset(input_data, \"Input_String\"),\n",
    "                batch_size=batch_size)\n",
    "    ):\n",
    "        pred_output.append(out)\n",
    "\n",
    "bert_inference(inference_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
