{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Notebook\n",
    "- Calculate the scores and format into table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, average_precision_score\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify parameters and import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model parameters\n",
    "model_name = \"Evaluation_Revised\"\n",
    "\n",
    "# --- Paths\n",
    "# Base data path\n",
    "base_data_path = Path(\"../00_Data/\")\n",
    "# Dataset Path (training, testing, etc.)\n",
    "dataset_path =  base_data_path / \"publication_ready\"\n",
    "# Model Path\n",
    "model_path = base_data_path / \"model_output\"\n",
    "# Export Path (model checkpoints, predictions, etc.)\n",
    "export_path = model_path / model_name\n",
    "\n",
    "assert base_data_path.is_dir(),\\\n",
    "  f\"{base_data_path} either doesn't exist or is not a directory.\"\n",
    "export_path.mkdir(exist_ok=True)\n",
    "\n",
    "# --- Misc settings\n",
    "# Print dataframes and keys to debug\n",
    "debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['urinary',  'respiratory',  'abdominal',  'neurological', 'skin_soft_tissue', 'ent', 'orthopaedic',\n",
    "           'other_specific', 'no_specific_source', 'prophylaxis', 'uncertainty', 'not_informative']\n",
    "labels_pretty = []\n",
    "for label in labels:\n",
    "    if label == \"ent\":\n",
    "        labels_pretty.append(\"ENT\")\n",
    "        continue\n",
    "    labels_pretty.append(\" \".join(word.capitalize() for word in label.split(\"_\")))\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "labels2labels_pretty = {old:pretty for old, pretty in zip(labels, labels_pretty)}\n",
    "\n",
    "test_oxford_df = pd.read_csv(\n",
    "    dataset_path / 'testing_oxford_2023-08-23.csv',\n",
    "    dtype={\"Indication\": str},\n",
    "    keep_default_na=False,\n",
    "    na_values=[\"NA\"],\n",
    ").rename(\n",
    "    columns=labels2labels_pretty\n",
    ")\n",
    "\n",
    "test_banbury_df = pd.read_csv(\n",
    "    dataset_path / 'testing_banbury_2023-08-23.csv',\n",
    "    dtype={\"Indication\": str},\n",
    "    keep_default_na=False,\n",
    "    na_values=[\"NA\"],\n",
    ").rename(\n",
    "    columns=labels2labels_pretty\n",
    ")\n",
    "\n",
    "test_set_raw = {\n",
    "    \"Oxford\": test_oxford_df,\n",
    "    \"Banbury\": test_banbury_df,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dict = {\n",
    "    \"Regex\": {\n",
    "        \"has_proba\": False,\n",
    "    },\n",
    "\n",
    "    \"XGBoost\": {\n",
    "        \"has_proba\": True,\n",
    "    },\n",
    "    \n",
    "    \"Base_BERT\": {\n",
    "        \"has_proba\": True,\n",
    "    },\n",
    "\n",
    "    \"Bio_ClinicalBERT\": {\n",
    "        \"has_proba\": True,\n",
    "    },\n",
    "\n",
    "    \"GPT3.5\": {\n",
    "        \"has_proba\": False,\n",
    "        \"file_location\": \"Gpt-3.5-turbo-0125-Finetuned-Json\",\n",
    "        \"file_descriptor\": \"ft\"\n",
    "    },\n",
    "\n",
    "    \"GPT4\": {\n",
    "        \"has_proba\": False,\n",
    "        \"file_location\": \"Gpt-4-0125-preview-Zero_Shot-Json\",\n",
    "        \"file_descriptor\": \"zs\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_class_accuracy(y_true, y_pred):\n",
    "    class_accuracies = []\n",
    "    # Assuming y_true and y_pred are numpy arrays and have shape (n_samples, n_classes)\n",
    "    n_classes = y_true.shape[1]\n",
    "    for i in range(n_classes):\n",
    "        class_accuracies.append(accuracy_score(y_true.iloc[:, i], y_pred.iloc[:, i]))\n",
    "    return class_accuracies\n",
    "\n",
    "def weighted_accuracy(y_true, class_accuracies):\n",
    "    \"\"\"Calculate the weighted average based on the class distribution.\n",
    "    The results agree with sklean's \"average=weighted\" method.\n",
    "    \"\"\"\n",
    "    # Calculate the frequency of each class\n",
    "    class_counts = np.sum(y_true, axis=0)\n",
    "    # Calculate the total number of samples\n",
    "    total_samples = np.sum(class_counts)\n",
    "    # Calculate weights for each class\n",
    "    weights = class_counts / total_samples\n",
    "    # Calculate the weighted average accuracy\n",
    "    weighted_avg_accuracy = np.sum(weights * class_accuracies)\n",
    "    return weighted_avg_accuracy\n",
    "\n",
    "\n",
    "# Metrics function\n",
    "def calculate_metrics(y_true, \n",
    "                      predictions_probs,\n",
    "                      predictions_binarised,\n",
    "                      labels, \n",
    "                      result_precision=2, \n",
    "                      averaging_method = \"weighted\",\n",
    "                      n_iter=1000,\n",
    "                      ci=0.95,\n",
    "    ):\n",
    "    # Calculate per class metrics\n",
    "    scores_per_class_dict = {}\n",
    "    scores_per_class_dict[\"F1-Score\"] = f1_score(y_true=y_true, y_pred=predictions_binarised, average=None)\n",
    "    scores_per_class_dict[\"ROC AUC\"] = roc_auc_score(y_true=y_true, y_score=predictions_probs, average=None)\n",
    "    scores_per_class_dict[\"PR AUC\"] = average_precision_score(y_true=y_true, y_score=predictions_probs, average=None)\n",
    "    scores_per_class_dict[\"Accuracy\"] = per_class_accuracy(y_true=y_true, y_pred=predictions_binarised)\n",
    "    scores_per_class_dict[\"Total Accuracy\"] = []\n",
    "\n",
    "    scores_per_class = pd.DataFrame.from_dict(scores_per_class_dict,orient='index', columns=labels)\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    scores_average = {}\n",
    "    scores_average[\"F1-Score\"] = f1_score(y_true=y_true, y_pred=predictions_binarised, average=averaging_method)\n",
    "    scores_average[\"ROC AUC\"] = roc_auc_score(y_true=y_true, y_score=predictions_probs, average=averaging_method)\n",
    "    scores_average[\"PR AUC\"] = average_precision_score(y_true=y_true, y_score=predictions_probs, average=averaging_method)\n",
    "    scores_average[\"Accuracy\"] = weighted_accuracy(y_true=y_true, class_accuracies=scores_per_class_dict[\"Accuracy\"])\n",
    "    scores_average[\"Total Accuracy\"] = np.float64(accuracy_score(y_true=y_true, y_pred=predictions_binarised))\n",
    "\n",
    "    # Calculate 95 CI based on bootstrapping\n",
    "    iter_scores = {\n",
    "        \"F1-Score\": [],\n",
    "        \"ROC AUC\": [],\n",
    "        \"PR AUC\": [],\n",
    "        \"Accuracy\": [],\n",
    "        \"Total Accuracy\": [],\n",
    "    }\n",
    "\n",
    "    for _ in range(n_iter):\n",
    "        # Resample with replacement from the original training data\n",
    "        y_true_resampled, y_pred_probs, y_pred_bin_resampled = resample(y_true, predictions_probs, predictions_binarised, replace=True)\n",
    "\n",
    "        # Calculate the metric of interest\n",
    "        iter_scores[\"F1-Score\"].append(f1_score(y_true=y_true_resampled, y_pred=y_pred_bin_resampled, average=averaging_method))\n",
    "        iter_scores[\"ROC AUC\"].append(roc_auc_score(y_true=y_true_resampled, y_score=y_pred_probs, average=averaging_method))\n",
    "        iter_scores[\"PR AUC\"].append(average_precision_score(y_true=y_true_resampled, y_score=y_pred_probs, average=averaging_method))\n",
    "        iter_scores[\"Accuracy\"].append(weighted_accuracy(y_true=y_true_resampled, class_accuracies=per_class_accuracy(y_true=y_true_resampled, y_pred=y_pred_bin_resampled)))\n",
    "        iter_scores[\"Total Accuracy\"].append(accuracy_score(y_true=y_true_resampled, y_pred=y_pred_bin_resampled))\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    scores_ci = {}\n",
    "    ci = (1 - ci) / 2\n",
    "    for score_name, score_values in iter_scores.items():\n",
    "        scores_ci[score_name] = np.percentile(score_values, [ci * 100, (1 - ci) * 100])\n",
    "\n",
    "    \n",
    "    \n",
    "    # Format into printable string\n",
    "    metrics_string = \"\"\n",
    "    for score_name, avg_score_value in scores_average.items():\n",
    "        avg_score = avg_score_value.round(result_precision)\n",
    "        min_sore = scores_per_class.loc[score_name].min().round(result_precision)\n",
    "        max_score = scores_per_class.loc[score_name].max().round(result_precision) \n",
    "        metrics_string += f\"{score_name}: {avg_score} ({min_sore}-{max_score})\\n\"\n",
    "    \n",
    "    return scores_per_class, scores_average, scores_ci, metrics_string\n",
    "\n",
    "# for location, y_test_true in test_set_raw.items():\n",
    "#     y_test_true = test_banbury_df[labels_pretty]\n",
    "#     y_pred = pd.read_csv(\"/home/kevin/DPhil/Projects/EHR-Indication-Processing/00_Data/model_output/Bio_ClinicalBERT/predictions_Bio_ClinicalBERT_Banbury.csv\").rename(columns=labels2labels_pretty)[labels_pretty]\\\n",
    "#             .fillna(0)\n",
    "#     d = calculate_metrics(y_test_true, y_pred, y_pred, labels_pretty)\n",
    "#     print(d)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full result table (main section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Regex, Location: Oxford, Category: Not Informative\n",
      "[[1947   19]\n",
      " [  34    0]]\n",
      "Model: Regex, Location: Banbury, Category: Not Informative\n",
      "[[1956    9]\n",
      " [  35    0]]\n"
     ]
    }
   ],
   "source": [
    "# --- Repeat for each location\n",
    "for location, y_test_true in test_set_raw.items():\n",
    "    y_test_true = y_test_true[labels_pretty]\n",
    "\n",
    "    # --- Repeat for each model\n",
    "    scores_full_list = []\n",
    "    for model_name, metadata in predictions_dict.items():\n",
    "        # --- Build filepaths and parse CSVs\n",
    "        file_folder = metadata[\"file_location\"] if (\"file_location\" in metadata) else model_name\n",
    "        file_descriptor = metadata[\"file_descriptor\"] if (\"file_descriptor\" in metadata) else model_name\n",
    "        file_proba_tag = \"_proba\" if metadata[\"has_proba\"] else \"\"\n",
    "\n",
    "        predictions_bin_path = model_path / file_folder / f\"predictions_{file_descriptor}_{location}.csv\"\n",
    "        predictions_prob_path = model_path / file_folder / f\"predictions{file_proba_tag}_{file_descriptor}_{location}.csv\"\n",
    "\n",
    "        predictions_binarised = pd.read_csv(predictions_bin_path)\\\n",
    "            .rename(columns=labels2labels_pretty)[labels_pretty]\\\n",
    "            .fillna(0)\n",
    "        predictions_proba = pd.read_csv(predictions_prob_path)\\\n",
    "            .rename(columns=labels2labels_pretty)[labels_pretty]\\\n",
    "            .fillna(0)\n",
    "        \n",
    "        if model_name == \"Regex\":\n",
    "            from sklearn.metrics import confusion_matrix\n",
    "            print(f\"Model: {model_name}, Location: {location}, Category: Not Informative\")\n",
    "            print(confusion_matrix(y_true=y_test_true[\"Not Informative\"], y_pred=predictions_binarised[\"Not Informative\"]))\n",
    "\n",
    "        # Calculate the score\n",
    "        scores_per_class, scores_average, scores_ci, metrics_string = calculate_metrics(y_test_true, predictions_proba, predictions_binarised, labels_pretty, averaging_method=\"weighted\")\n",
    "        scores_per_class[\"Avg\"] = scores_average\n",
    "        scores_per_class[\"Min\"] = scores_per_class.min(axis=1)\n",
    "        scores_per_class[\"Max\"] = scores_per_class.max(axis=1)\n",
    "        scores_per_class[\"Str\"] = scores_per_class\\\n",
    "            .apply(lambda row: f\"{row['Avg']:.2f} [{row['Min']:.2f}-{row['Max']:.2f}]\", axis=1)\n",
    "        scores_per_class[\"CI\"] = {key: np.array2string(value, precision=2, separator=\",\") for key, value in scores_ci.items()}\n",
    "\n",
    "\n",
    "        # Add metadata back as columns\n",
    "        scores_per_class = scores_per_class\\\n",
    "            .rename_axis('Metric')\\\n",
    "            .reset_index()\n",
    "        scores_per_class\\\n",
    "            .insert(loc=0, column='Model', value=model_name)\n",
    "        \n",
    "        scores_full_list.append(scores_per_class)\n",
    "\n",
    "    scores_full = pd.concat(scores_full_list)\n",
    "\n",
    "    scores_full.to_csv(export_path / f\"scores_full_{location}.csv\", index=False, float_format='%.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table for supplements (with micro, macro average) [very ugly]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Regex, Location: Oxford, Category: Not Informative\n",
      "[[1947   19]\n",
      " [  34    0]]\n",
      "Model: Regex, Location: Banbury, Category: Not Informative\n",
      "[[1956    9]\n",
      " [  35    0]]\n"
     ]
    }
   ],
   "source": [
    "# --- Repeat for each location\n",
    "for location, y_test_true in test_set_raw.items():\n",
    "    y_test_true = y_test_true[labels_pretty]\n",
    "\n",
    "    # --- Repeat for each model\n",
    "    scores_full_list = []\n",
    "    for model_name, metadata in predictions_dict.items():\n",
    "        # --- Build filepaths and parse CSVs\n",
    "        file_folder = metadata[\"file_location\"] if (\"file_location\" in metadata) else model_name\n",
    "        file_descriptor = metadata[\"file_descriptor\"] if (\"file_descriptor\" in metadata) else model_name\n",
    "        file_proba_tag = \"_proba\" if metadata[\"has_proba\"] else \"\"\n",
    "\n",
    "        predictions_bin_path = model_path / file_folder / f\"predictions_{file_descriptor}_{location}.csv\"\n",
    "        predictions_prob_path = model_path / file_folder / f\"predictions{file_proba_tag}_{file_descriptor}_{location}.csv\"\n",
    "\n",
    "        predictions_binarised = pd.read_csv(predictions_bin_path)\\\n",
    "            .rename(columns=labels2labels_pretty)[labels_pretty]\\\n",
    "            .fillna(0)\n",
    "        predictions_proba = pd.read_csv(predictions_prob_path)\\\n",
    "            .rename(columns=labels2labels_pretty)[labels_pretty]\\\n",
    "            .fillna(0)\n",
    "        \n",
    "        if model_name == \"Regex\":\n",
    "            from sklearn.metrics import confusion_matrix\n",
    "            print(f\"Model: {model_name}, Location: {location}, Category: Not Informative\")\n",
    "            print(confusion_matrix(y_true=y_test_true[\"Not Informative\"], y_pred=predictions_binarised[\"Not Informative\"]))\n",
    "\n",
    "        # Calculate the score\n",
    "        scores_per_class, scores_average_weighted, scores_ci, metrics_string = calculate_metrics(y_test_true, predictions_proba, predictions_binarised, labels_pretty, averaging_method=\"weighted\", n_iter=1)\n",
    "        _, scores_average_micro , _, _ = calculate_metrics(y_test_true, predictions_proba, predictions_binarised, labels_pretty, averaging_method=\"micro\", n_iter=1)\n",
    "        _, scores_average_macro, _, _ = calculate_metrics(y_test_true, predictions_proba, predictions_binarised, labels_pretty, averaging_method=\"macro\", n_iter=1)\n",
    "        scores_per_class[\"Avg Weighted\"] = scores_average_weighted\n",
    "        scores_per_class[\"Avg Micro\"] = scores_average_micro\n",
    "        scores_per_class[\"Avg Macro\"] = scores_average_macro \n",
    "        \n",
    "\n",
    "\n",
    "        # Add metadata back as columns\n",
    "        scores_per_class = scores_per_class\\\n",
    "            .rename_axis('Metric')\\\n",
    "            .reset_index()\n",
    "        scores_per_class\\\n",
    "            .insert(loc=0, column='Model', value=model_name)\n",
    "        \n",
    "        scores_full_list.append(scores_per_class)\n",
    "\n",
    "    scores_full = pd.concat(scores_full_list)\n",
    "\n",
    "    scores_full.to_csv(export_path / f\"scores_appendix_{location}.csv\", index=False, float_format='%.2f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
