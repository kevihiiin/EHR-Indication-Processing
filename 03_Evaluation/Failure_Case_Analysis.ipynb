{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Failure Case Analysis\n",
    "Visualise the misclasified samples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kevin/DPhil/Projects/EHR-Indication-Processing/03_Evaluation\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Print information about the current working directory\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"../00_Data/export\")\n",
    "\n",
    "model_name = \"Bio_ClinicalBERT\"\n",
    "\n",
    "bert_predictions_path = data_path/f\"{model_name}_predictions.csv\"\n",
    "bert_true_path = data_path/f\"{model_name}_true_labels.csv\"\n",
    "\n",
    "bert_true = pd.read_csv(bert_true_path)\n",
    "bert_predictions = pd.read_csv(bert_predictions_path)\n",
    "\n",
    "# Set indicies\n",
    "bert_true = bert_true.set_index('IndicationRaw')\n",
    "bert_predictions = bert_predictions.set_index(bert_true.index)\n",
    "\n",
    "# Convert to booleans\n",
    "bert_true = bert_true.astype(int)\n",
    "bert_predictions = bert_predictions.astype(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the failure cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_column = \"Reference\"\n",
    "predicted_column = \"Predicted\"\n",
    "compared = bert_true.compare(bert_predictions, result_names=(reference_column, predicted_column), keep_equal=True).dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_differences(s):\n",
    "    row_style = []\n",
    "    highlight_format_true = \"color:green; font-weight:bold\"\n",
    "    highlight_format_false = \"color:red; font-weight:bold\"\n",
    "\n",
    "    # Iterate over the first index (true labels) and set colour if not identical\n",
    "    for level_value in s.index.get_level_values(0).unique():\n",
    "        level_slice = s.loc[level_value, :]\n",
    "        if level_slice[reference_column] == level_slice[predicted_column]:\n",
    "            row_style += [None, None]\n",
    "        else:\n",
    "            row_style += [highlight_format_true, highlight_format_false]\n",
    "    \n",
    "    return row_style\n",
    "\n",
    "compared_highlighted = compared.style.apply(highlight_differences, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "compared_highlighted_path = data_path/f\"{model_name}_compared.xlsx\"\n",
    "compared_highlighted.to_excel(compared_highlighted_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
