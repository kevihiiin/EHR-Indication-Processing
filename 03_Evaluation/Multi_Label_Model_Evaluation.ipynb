{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model performance\n",
    "Create metrics for the multi-label output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Import libraries & set parameters, helper functions, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Sklearn imports for model evaluation\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns\n",
    "column_mapper = {\n",
    "    \"urinary\": \"Urinary\",\n",
    "    \"respiratory\": \"Respiratory\",\n",
    "    \"abdominal\": \"Abdominal\",\n",
    "    \"neurological\": \"Neurological\",\n",
    "    \"skin_soft_tissue\": \"Skin & Soft Tissue\",\n",
    "    \"ent\": \"ENT\",\n",
    "    \"orthopaedic\": \"Orthopaedic\",\n",
    "    \"other_specific\": \"Other Specific\",\n",
    "    \"no_specific_source\": \"No Specific Source\",\n",
    "    \"prophylaxis\": \"Prophylaxis\",\n",
    "    \"uncertainty\": \"Uncertainty\",\n",
    "    \"not_informative\": \"Not Informative\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categories\n",
    "categories = [\"Urinary\", \"Respiratory\", \"Abdominal\", \"Neurological\", \"Skin & Soft Tissue\", \"ENT\", \"Orthopaedic\", \"Other Specific\", \"No Specific Source\", \"Prophylaxis\", \"Uncertainty\", \"Not Informative\"]\n",
    "categories.sort()\n",
    "\n",
    "categoires_no_uncertainty = categories.copy()\n",
    "categoires_no_uncertainty.remove(\"Uncertainty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model names & properties\n",
    "model_has_proba = {\n",
    "    \"Regex\": False,\n",
    "    \"Base_Bert\": True,\n",
    "    \"Bio_ClinicalBERT\": True,\n",
    "    \"GPT4\": False,\n",
    "    \"GPT3.5\": False\n",
    "}\n",
    "\n",
    "locations = [\"oxford\", \"banbury\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting to: ../00_Data/export/performance/plots\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "data_dir = Path(\"../00_Data\")\n",
    "plot_dir = data_dir / \"export/performance/plots\"\n",
    "plot_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Exporting to:\", plot_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(data_dir, model_name, test_location, has_proba):\n",
    "    \"\"\"Import the data for the model predictions and the true values.\n",
    "    Returns the binary and probability predictions and the true values.\n",
    "    TODO: Implement importing has_proba\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    df_predictions = pl.read_csv(data_dir / 'export' / model_name / f'{model_name}_{test_location}_predictions.csv')\\\n",
    "        .rename(column_mapper)\n",
    "\n",
    "    df_predictions_binary = df_predictions.clone()\n",
    "    df_predictions_proba = df_predictions.clone()\n",
    "\n",
    "    df_true_full = pl.read_csv(data_dir / \"publication_ready\" / f\"testing_{test_location}_2023-08-23.csv\")\\\n",
    "        .drop(\"PrescriptionID\")\\\n",
    "        .rename(column_mapper)\n",
    "\n",
    "    df_true = df_true_full[categories]\n",
    "\n",
    "    # Return them all in the same order\n",
    "    return df_predictions_binary[categories], df_predictions_proba[categories],\\\n",
    "        df_true[categories], df_true_full\n",
    "\n",
    "# Import the data\n",
    "\n",
    "\n",
    "model_name = \"Bio_ClinicalBERT\"\n",
    "location = \"banbury\"\n",
    "\n",
    "\n",
    "df_pred_binary, df_pred_proba, df_true, df_true_full = import_data(data_dir, model_name, location, model_has_proba[model_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate per-class metrics (F1 Score and ROC AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 17)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Model Name</th><th>Metric</th><th>Abdominal</th><th>ENT</th><th>Neurological</th><th>No Specific Source</th><th>Not Informative</th><th>Orthopaedic</th><th>Other Specific</th><th>Prophylaxis</th><th>Respiratory</th><th>Skin &amp; Soft Tissue</th><th>Uncertainty</th><th>Urinary</th><th>Mean</th><th>Min</th><th>Max</th></tr><tr><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;Bio_ClinicalBE…</td><td>&quot;F1-Score&quot;</td><td>0.951965</td><td>0.962963</td><td>0.972973</td><td>0.988506</td><td>0.83871</td><td>0.883721</td><td>0.862745</td><td>0.984701</td><td>0.991453</td><td>0.958678</td><td>0.988506</td><td>0.981758</td><td>0.947223</td><td>0.83871</td><td>0.991453</td></tr><tr><td>&quot;Bio_ClinicalBE…</td><td>&quot;ROC AUC&quot;</td><td>0.961333</td><td>0.964286</td><td>0.999748</td><td>0.99245</td><td>0.871174</td><td>0.895833</td><td>0.938987</td><td>0.991208</td><td>0.992145</td><td>0.969168</td><td>0.996634</td><td>0.984656</td><td>0.963135</td><td>0.871174</td><td>0.999748</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 17)\n",
       "┌────────────────┬──────────┬───────────┬──────────┬───┬──────────┬──────────┬──────────┬──────────┐\n",
       "│ Model Name     ┆ Metric   ┆ Abdominal ┆ ENT      ┆ … ┆ Urinary  ┆ Mean     ┆ Min      ┆ Max      │\n",
       "│ ---            ┆ ---      ┆ ---       ┆ ---      ┆   ┆ ---      ┆ ---      ┆ ---      ┆ ---      │\n",
       "│ str            ┆ str      ┆ f64       ┆ f64      ┆   ┆ f64      ┆ f64      ┆ f64      ┆ f64      │\n",
       "╞════════════════╪══════════╪═══════════╪══════════╪═══╪══════════╪══════════╪══════════╪══════════╡\n",
       "│ Bio_ClinicalBE ┆ F1-Score ┆ 0.951965  ┆ 0.962963 ┆ … ┆ 0.981758 ┆ 0.947223 ┆ 0.83871  ┆ 0.991453 │\n",
       "│ RT             ┆          ┆           ┆          ┆   ┆          ┆          ┆          ┆          │\n",
       "│ Bio_ClinicalBE ┆ ROC AUC  ┆ 0.961333  ┆ 0.964286 ┆ … ┆ 0.984656 ┆ 0.963135 ┆ 0.871174 ┆ 0.999748 │\n",
       "│ RT             ┆          ┆           ┆          ┆   ┆          ┆          ┆          ┆          │\n",
       "└────────────────┴──────────┴───────────┴──────────┴───┴──────────┴──────────┴──────────┴──────────┘"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_per_class_list = []\n",
    "scores_per_class_list.append([model_name, \"F1-Score\"] + f1_score(y_true=df_true, y_pred=df_pred_binary, average=None).tolist())\n",
    "scores_per_class_list.append([model_name, \"ROC AUC\"] + roc_auc_score(y_true=df_true, y_score=df_pred_proba, average=None).tolist())\n",
    "\n",
    "# Convert to dataframe\n",
    "scores_per_class_df = pl.DataFrame(scores_per_class_list, schema=[\"Model Name\", \"Metric\"] + categories)\n",
    "\n",
    "# Calculate mean, min & max per row (model and metric)\n",
    "(scores_per_class_df\n",
    "    .with_columns(\n",
    "        pl.mean_horizontal(categories).alias(\"Average\"),\n",
    "        pl.min_horizontal(categories).alias(\"Min\"),\n",
    "        pl.max_horizontal(categories).alias(\"Max\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate overall averages (F1 Score and ROC AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'F1-Score': 0.9472231441366833,\n",
       " 'ROC AUC': 0.9631351966208479,\n",
       " 'Accuracy': 0.9645}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_average = {}\n",
    "averaging_method = \"macro\"\n",
    "scores_average[\"F1-Score\"] = f1_score(y_true=df_true, y_pred=df_pred_binary, average=averaging_method)\n",
    "scores_average[\"ROC AUC\"] = roc_auc_score(y_true=df_true, y_score=df_pred_proba, average=averaging_method)\n",
    "scores_average[\"Accuracy\"] = accuracy_score(y_true=df_true, y_pred=df_pred_binary)\n",
    "\n",
    "scores_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
