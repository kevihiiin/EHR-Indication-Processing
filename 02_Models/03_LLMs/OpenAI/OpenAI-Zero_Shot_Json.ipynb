{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Models - Zero Shot model\n",
    "Test the classification perfomance of OpenAI LLMs.\n",
    "\n",
    "Test cases will include:\n",
    "- **Zero Shot Models**\n",
    "- Embedding + XGBoost (or Cosine Similarity)\n",
    "- Finetuned model\n",
    "\n",
    "This notebook will attempt to create achieve multi-class classification by leaveraging the JSON output functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "- [x] The new preview model supports setting a seed to make \"reproducable\" runs\n",
    "- [ ] Run model multiple rounds and check reproducability\n",
    "- [x] Use .env files to set API keys\n",
    "- [x] Implement token counter and optimise supplied prompts\n",
    "- [x] Return number of used promts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Study Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries\n",
    "Load libraries and the API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current work directory: /home/kevin/DPhil/Projects/EHR-Indication-Processing/02_Models/03_LLMs/OpenAI\n"
     ]
    }
   ],
   "source": [
    "# --- Load libraries\n",
    "# Standard libraries\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pickle\n",
    "\n",
    "# Misc\n",
    "import jinja2\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# DS libs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# ML libs\n",
    "from openai import OpenAI\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "\n",
    "# --- Specify logging level\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# --- Check the environment and load API key\n",
    "print(\"Current work directory:\", Path.cwd())\n",
    "\n",
    "# Load API key (DO NOT HARDCODE)\n",
    "load_dotenv()\n",
    "\n",
    "if _SECRET_KEY := os.getenv(\"OPENAI_API_KEY\"):\n",
    "    logging.debug(\"API key found.\")\n",
    "    client = OpenAI(\n",
    "        # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "        api_key=_SECRET_KEY\n",
    "    )\n",
    "else:\n",
    "    logging.error(\"API key not found. Please set the environment variable OPENAI_API_KEY\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Study Parameters\n",
    "Data paths and model to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "model_selection = \"GPT4\"  # \"Davinci\" or \"Curie\" or \"Babbage\" or \"Ada\"\n",
    "\n",
    "model_dict = {\n",
    "    \"GPT4\": \"gpt-4-0125-preview\",\n",
    "    # Only the following are made for chats\n",
    "    \"GPT3.5 Turbo\": \"gpt-3.5-turbo\",\n",
    "    # Only the following supports pure completion and text substitution\n",
    "    \"GPT3.5 Davinci\": \"text-davinci-003\",\n",
    "    # Only the following support finetuning, decreasing in performance and cost\n",
    "    \"Davinci\": \"davinci\",\n",
    "    \"Curie\": \"curie\",\n",
    "    \"Babbage\": \"babbage\",\n",
    "    \"Ada\": \"ada\",    \n",
    "}\n",
    "\n",
    "# --- Misc settings\n",
    "# Model names\n",
    "model_name_display = model_selection\n",
    "model_openai_id = model_dict[model_selection]  # OpenAI name/identifier\n",
    "\n",
    "# --- Paths\n",
    "# Base data path\n",
    "base_data_path = Path(\"../../../00_Data/\")\n",
    "# Dataset Path (training, testing, etc.)\n",
    "dataset_path =  base_data_path / \"publication_ready\"\n",
    "# Export Path (model checkpoints, predictions, etc.)\n",
    "export_path = base_data_path / \"model_output\" / f\"{model_openai_id.capitalize()}-Zero_Shot-Json\"\n",
    "\n",
    "\n",
    "assert base_data_path.is_dir(),\\\n",
    "  f\"{base_data_path} either doesn't exist or is not a directory.\"\n",
    "export_path.mkdir(exist_ok=True)\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and clean data\n",
    "Import the test and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set size overview:\n",
      "- Training set: 3400\n",
      "- Evaluation set: 600\n",
      "- Testing Oxford set: 2000\n",
      "- Testing Banbury set: 2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import data --> upload into \"Files\" on the left-hand panel\n",
    "train_eval_df = pd.read_csv(\n",
    "    dataset_path / 'training_oxford_2023-08-23.csv',\n",
    "    dtype={\"Indication\": str},\n",
    "    keep_default_na=False,\n",
    "    na_values=[\"NA\"],\n",
    ")\n",
    "\n",
    "test_oxford_df = pd.read_csv(\n",
    "    dataset_path / 'testing_oxford_2023-08-23.csv',\n",
    "    dtype={\"Indication\": str},\n",
    "    keep_default_na=False,\n",
    "    na_values=[\"NA\"],\n",
    ")\n",
    "\n",
    "test_banbury_df = pd.read_csv(\n",
    "    dataset_path / 'testing_banbury_2023-08-23.csv',\n",
    "    dtype={\"Indication\": str},\n",
    "    keep_default_na=False,\n",
    "    na_values=[\"NA\"],\n",
    ")\n",
    "\n",
    "# --- Split into train and eval\n",
    "train_df, eval_df = train_test_split(\n",
    "    train_eval_df, \n",
    "    test_size=0.15,\n",
    "    random_state=42,\n",
    "    shuffle=True)\n",
    "\n",
    "print(\"Data set size overview:\")\n",
    "print(f\"- Training set: {train_df.shape[0]}\")\n",
    "print(f\"- Evaluation set: {eval_df.shape[0]}\")\n",
    "print(f\"- Testing Oxford set: {test_oxford_df.shape[0]}\")\n",
    "print(f\"- Testing Banbury set: {test_banbury_df.shape[0]}\")\n",
    "print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define labels and mappers\n",
    "Convert labels to numbers and get prettier labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Urinary',\n",
       " 'Respiratory',\n",
       " 'Abdominal',\n",
       " 'Neurological',\n",
       " 'Skin Soft Tissue',\n",
       " 'ENT',\n",
       " 'Orthopaedic',\n",
       " 'Other Specific',\n",
       " 'No Specific Source',\n",
       " 'Prophylaxis',\n",
       " 'Uncertainty',\n",
       " 'Not Informative']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels\n",
    "labels = [label for label in train_df.columns if label not in [\"Indication\"]]\n",
    "labels_pretty = []\n",
    "for label in labels:\n",
    "    if label == \"ent\":\n",
    "        labels_pretty.append(\"ENT\")\n",
    "        continue\n",
    "    labels_pretty.append(\" \".join(word.capitalize() for word in label.split(\"_\")))\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "labels2labels_pretty = {old:pretty for old, pretty in zip(labels, labels_pretty)}\n",
    "\n",
    "labels_pretty"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data\n",
    "\n",
    "- Prettyfy the column labels (rename them)\n",
    "- Get a subset of the data for experimenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [train_df, eval_df, test_oxford_df, test_banbury_df]:\n",
    "    dataset.rename(columns=labels2labels_pretty, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now get a subset of the training data:\n",
    "- Extract some indications as validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subsample = train_df.sample(n=100)\n",
    "test_subsample_indications = test_subsample.Indication"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot Model\n",
    "Create a good query to use for Zero Shot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_completion(client, system_prompt, user_prompt, model_openai_id, max_tokens=100, seed=42):\n",
    "    \"\"\"Sends the promt to the OpenAI API and returns the response.\n",
    "    Specify parameters for the model in the function call.\n",
    "    \"\"\"\n",
    "    # --- Fetch Chat Completion\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt,\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,  # Set lower temperature (default 0)\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=1,  # Return only the most likely completion (save tokens)\n",
    "        frequency_penalty=0,  # Default of 0, repeating sequences are ok and wanted\n",
    "        presence_penalty=0,  # Set to lower value to decrease the likelyhood if the model inveting new words/categories\n",
    "        model=model_openai_id,\n",
    "        response_format={ \"type\": \"json_object\" },\n",
    "        seed=seed,  # Set seed for reproducibility (check the API documentation for more details)\n",
    "        #logit_bias  # Force the model to only reply with the specified labels?\n",
    "        #logprobs  # Can it be used for evaluation?\n",
    "    )\n",
    "    # --- Process the response\n",
    "    # -- Content\n",
    "    # There will only be one completion given the parameter `top_p=1`\n",
    "    chat_completion_content = chat_completion.choices[0]\n",
    "\n",
    "    # Check whether the completion was truncated\n",
    "    if (finish_reason := chat_completion_content.finish_reason) != \"stop\":\n",
    "        logging.warning(f\"Completion was truncated. Finish reason: {finish_reason}\")\n",
    "    \n",
    "    chat_completion_message = chat_completion_content.message.content\n",
    "\n",
    "    # -- Metadata\n",
    "    # Gather general metadata\n",
    "    chat_completion_metadata = {\n",
    "        \"model\": chat_completion.model,\n",
    "        \"created\": chat_completion.created,\n",
    "        \"finish_reason\": finish_reason,\n",
    "        \"system_fingerprint\": chat_completion.system_fingerprint,\n",
    "    }\n",
    "\n",
    "    # Get usage metadata\n",
    "    chat_completion_usage = {\n",
    "        \"completion_tokens\": chat_completion.usage.completion_tokens,\n",
    "        \"prompt_tokens\": chat_completion.usage.prompt_tokens,\n",
    "        \"total_tokens\": chat_completion.usage.total_tokens,\n",
    "    }\n",
    "    \n",
    "    return chat_completion_message, chat_completion_metadata, chat_completion_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Request formatting\n",
    "The new API (November 2023) allows/requests to specify three messages:\n",
    "1. System Prompt: Task description\n",
    "2. User Prompt User input\n",
    "3. Assistant Prompt: Model response\n",
    "\n",
    "The system prompt is the same for each request (static).\n",
    "The user prompt is dynamically generated and reformats the input into a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_system_template_string = \"\"\"You are a helpful and precise UK medical expert. You have been given a list of indications describing why antibiotics were prescribed to patients in a hospital and asked to label these indications into categories.\n",
    "You can only choose from the following categories: {% for category in categories %}\"{{ category }}\"{% if not loop.last %}, {% endif %}{% endfor %}.\n",
    "Multiple categories are allowed.\n",
    "\"ENT\" stands for \"Ear Nose and Throat\".\n",
    "\"Uncertainty\" refers to uncertainty specified by the clinician (e.g. \"?\" or multiple unrelated sources).\n",
    "\"No Spefic Source\" means a source can be inferred but it's not specific (e.g. just the word \"sepsis\" or \"infection\").\n",
    "\"Not Informative\" means the field does not reveal the source, is a viral infection or is unrelated to bacterial infections. When answering the question, please return a JSON.\n",
    "\"\"\"\n",
    "\n",
    "prompt_user_template_string = \\\n",
    "\"\"\"\n",
    "Return a JSON with the categories (multiple allowed) for each indication. \n",
    "Do not change or remove the supplied indications (dictionary key); only fill the empty arrays with the source categories specified above:\n",
    "{\n",
    "{% for indication in indications -%}\n",
    "\"{{ indication }}\":[],\n",
    "{% endfor %}\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Build the template\n",
    "environment = jinja2.Environment()\n",
    "prompt_user_template = environment.from_string(prompt_user_template_string)\n",
    "prompt_system_template = environment.from_string(prompt_system_template_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Render tempalte with example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Prompt:\n",
      "You are a helpful and precise UK medical expert. You have been given a list of indications describing why antibiotics were prescribed to patients in a hospital and asked to label these indications into categories.\n",
      "You can only choose from the following categories: \"Urinary\", \"Respiratory\", \"Abdominal\", \"Neurological\", \"Skin Soft Tissue\", \"ENT\", \"Orthopaedic\", \"Other Specific\", \"No Specific Source\", \"Prophylaxis\", \"Uncertainty\", \"Not Informative\".\n",
      "Multiple categories are allowed.\n",
      "\"ENT\" stands for \"Ear Nose and Throat\".\n",
      "\"Uncertainty\" refers to uncertainty specified by the clinician (e.g. \"?\" or multiple unrelated sources).\n",
      "\"No Spefic Source\" means a source can be inferred but it's not specific (e.g. just the word \"sepsis\" or \"infection\").\n",
      "\"Not Informative\" means the field does not reveal the source, is a viral infection or is unrelated to bacterial infections. When answering the question, please return a JSON.\n",
      "User Prompt:\n",
      "\n",
      "Return a JSON with the categories (multiple allowed) for each indication. \n",
      "Do not change or remove the supplied indications (dictionary key); only fill the empty arrays with the source categories specified above:\n",
      "{\n",
      "\"appendicities\":[],\n",
      "\"mai\":[],\n",
      "\"pyeloneph\":[],\n",
      "\"gallstone pancreatitis\":[],\n",
      "\"cellulitis right arm\":[],\n",
      "\"sepsis pn\":[],\n",
      "\"? pneumonia\":[],\n",
      "\"vulval haematoma\":[],\n",
      "\"c diff?\":[],\n",
      "\"likely uti\":[],\n",
      "\"post op pneumonia\":[],\n",
      "\"tb prophylaxis\":[],\n",
      "\"post-op appendicitis\":[],\n",
      "\"post cs pyrexia\":[],\n",
      "\"for thoracoscopy\":[],\n",
      "\"neutropeanic sepsis\":[],\n",
      "\"catheter change, uti\":[],\n",
      "\"rectal perforation\":[],\n",
      "\"staph aureus infection\":[],\n",
      "\"prophylaxis for uti\":[],\n",
      "\"bowel sepsis\":[],\n",
      "\"post procedure\":[],\n",
      "\"neutropaenic fevers\":[],\n",
      "\"reg meds\":[],\n",
      "\"cmv prophylaxis post tx\":[],\n",
      "\"infected shoulder\":[],\n",
      "\"device implantation\":[],\n",
      "\"cf psa\":[],\n",
      "\"infected metalwork\":[],\n",
      "\"intestinal perforation\":[],\n",
      "\"aspiration pneumonia/hap\":[],\n",
      "\"sigmoid diverticulitis\":[],\n",
      "\"cvid\":[],\n",
      "\"urine/chest infection\":[],\n",
      "\"neutropenic fever\":[],\n",
      "\"cf prophalaxis\":[],\n",
      "\"pinna abscess\":[],\n",
      "\"gram neg sepsis\":[],\n",
      "\"influenza a positive\":[],\n",
      "\"aspiration pnuemonia\":[],\n",
      "\"vulval cellulitis\":[],\n",
      "\"pre-patellar bursitis\":[],\n",
      "\"lrti.\":[],\n",
      "\"bowel transplant surgery\":[],\n",
      "\"uti, low egfr\":[],\n",
      "\"prostate embolisation\":[],\n",
      "\"pelvic collections\":[],\n",
      "\"intra abdo\":[],\n",
      "\"sepsis likely urine\":[],\n",
      "\"yeast in blood culture\":[],\n",
      "\"e. coli bacteraemia\":[],\n",
      "\"chorio\":[],\n",
      "\"prophylaxis during chemo\":[],\n",
      "\"left axillary abscess\":[],\n",
      "\"surgical prophylaxis\":[],\n",
      "\"surgery prophylaxis\":[],\n",
      "\"spc change\":[],\n",
      "\"possible cns infection\":[],\n",
      "\"uti catheter associated\":[],\n",
      "\"lower uti\":[],\n",
      "\"covid\":[],\n",
      "\"prolonged srom\":[],\n",
      "\"endometritis\":[],\n",
      "\"syphilis\":[],\n",
      "\"sepsis.\":[],\n",
      "\"left hand infection\":[],\n",
      "\"temp spikes\":[],\n",
      "\"pseudoaneurysm\":[],\n",
      "\"cap - sepsis\":[],\n",
      "\"h pylori eradication\":[],\n",
      "\"exacerbation\":[],\n",
      "\"pre admission\":[],\n",
      "\"tonsillitits\":[],\n",
      "\"catheter prophylaxis\":[],\n",
      "\"?cdt\":[],\n",
      "\"asthma\":[],\n",
      "\"traumatic catheter\":[],\n",
      "\"enterococci in urine\":[],\n",
      "\"? fungal infection\":[],\n",
      "\"hsv meningitis\":[],\n",
      "\"e faecium\":[],\n",
      "\"colovesicular fistula\":[],\n",
      "\"left parotitis\":[],\n",
      "\"post fess\":[],\n",
      "\"enterobacter uti\":[],\n",
      "\"infected laceration\":[],\n",
      "\"atypical cap\":[],\n",
      "\"pre device\":[],\n",
      "\"pre cardiac device\":[],\n",
      "\"as per plastics\":[],\n",
      "\"appendicits\":[],\n",
      "\"prokinetics\":[],\n",
      "\"sd empyema\":[],\n",
      "\"flexor sheath\":[],\n",
      "\"transplant prophylaxis\":[],\n",
      "\"urinary infection\":[],\n",
      "\"aspiration pneum\":[],\n",
      "\"infected joint\":[],\n",
      "\"see above\":[],\n",
      "\"?pseudomonas\":[],\n",
      "\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Render and display the tempaltes\n",
    "prompt_system = prompt_system_template.render(categories=labels_pretty)\n",
    "prompt_user = prompt_user_template.render(indications=test_subsample_indications, categories=labels_pretty)\n",
    "\n",
    "print(\"System Prompt:\")\n",
    "print(prompt_system)\n",
    "print(\"User Prompt:\")\n",
    "print(prompt_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the JSON return message into a indicator dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_message_to_df(return_msg_str):\n",
    "    # Convert the string to a dict/json\n",
    "    return_msg_json = json.loads(return_msg_str)\n",
    "\n",
    "    # Convert the dict to a DataFrame\n",
    "    return_msg_df = pd.DataFrame.from_dict(return_msg_json, orient='index')\n",
    "    input_index = return_msg_df.index\n",
    "\n",
    "    # Apply get_dummies and sum along the columns axis, to make indicator matrix\n",
    "    return_msg_df = pd.get_dummies(return_msg_df.stack().reset_index(level=1, drop=True))\\\n",
    "        .groupby(level=0, sort=False)\\\n",
    "        .sum()\\\n",
    "        .reindex(input_index)\n",
    "\n",
    "    return return_msg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename columns and sort order for the true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_categories = [category for category in labels_pretty if category not in [\"No Specific Source\", \"Not Informative\"]]\n",
    "metric_categories = [category for category in labels_pretty if category not in [\"Not Informative\"]]\n",
    "\n",
    "def score_response(pred_y, true_y, metric_categories, index_key=None):\n",
    "    # Reindex (rearrange) if specifided\n",
    "    if index_key:\n",
    "        # Set index\n",
    "        true_y = true_y.set_index(index_key)\n",
    "        pred_y = pred_y.set_index(index_key)\n",
    "        # Rearrange\n",
    "        pred_y = pred_y.reindex(true_y.index)\n",
    "\n",
    "    # --- Get true labels\n",
    "    y_test_pred = pred_y[metric_categories]\n",
    "    y_test_true = true_y[metric_categories]\n",
    "    # --- Calculate per-class metrics (F1 Score and ROC AUC)\n",
    "    scores_per_class = {}\n",
    "    scores_per_class[\"F1-Score\"] = f1_score(y_true=y_test_true, y_pred=y_test_pred, average=None)\n",
    "\n",
    "    scores_per_class = pd.DataFrame.from_dict(scores_per_class,orient='index', columns=metric_categories)\n",
    "\n",
    "    pd.set_option('display.precision', 2)\n",
    "\n",
    "    # --- Calculate overall averages (F1 Score and ROC AUC)\n",
    "    scores_average = {}\n",
    "    averaging_method = \"weighted\"\n",
    "    scores_average[\"F1-Score\"] = f1_score(y_true=y_test_true, y_pred=y_test_pred, average=averaging_method)\n",
    "    return scores_average, scores_per_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run for the entire dataset\n",
    "Use the defined methods and run the model for the whole dataset, chunk it!\n",
    "Add some smartness for reruns and error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data for code development\n",
    "train_pretty = train_df.rename(columns=labels2labels_pretty)[:200]\n",
    "train_pretty_indication = train_pretty.Indication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_completion(input_indications, categories, model_id, chunksize=100, reduce_usage=True):\n",
    "    # Reduce the number of input indications if specified\n",
    "    indications_orig = input_indications.copy()\n",
    "    if reduce_usage:\n",
    "        input_indications.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Variables to store the results & precomute stuff for the while loop\n",
    "    input_df_length = len(input_indications)\n",
    "\n",
    "    cursor = 0\n",
    "    tmp_prediction_df_list = []\n",
    "    prediction_metadata_list = []\n",
    "\n",
    "    # Setup progress bar\n",
    "    with tqdm(total=input_df_length) as p_bar:\n",
    "        # Start batch processing\n",
    "        while cursor < input_df_length:\n",
    "            cursor_end = min(cursor+chunksize, input_df_length)\n",
    "\n",
    "            logging.info(f\"Processing chunk {cursor}:{cursor_end} of {input_df_length}\")\n",
    "\n",
    "            # Subset the dataset\n",
    "            chunk_indications = input_indications.iloc[cursor:cursor_end]\n",
    "\n",
    "            # Render the templates\n",
    "            prompt_user = prompt_user_template.render(indications=chunk_indications, categories=categories)\n",
    "            prompt_system = prompt_system_template.render(categories=categories)\n",
    "\n",
    "            # Request the completion\n",
    "            chat_completion_message, chat_completion_metadata, chat_completion_usage = request_completion(\n",
    "                client=client,\n",
    "                system_prompt=prompt_system, \n",
    "                user_prompt=prompt_user, \n",
    "                model_openai_id=model_id, \n",
    "                max_tokens=None)\n",
    "            \n",
    "            # Check if output is truncated, reduce the maximum chunksize and rerun\n",
    "            if chat_completion_metadata[\"finish_reason\"] != \"stop\":\n",
    "                chunksize = chunksize - 10\n",
    "                logging.warning(f\"Maximum chunksize has been reduced to {chunksize}\")\n",
    "\n",
    "                if chunksize <=0:\n",
    "                    logging.error(f\"The chunksize {chunksize} is not reachable. Please investigate the input\")\n",
    "                    break\n",
    "                continue\n",
    "\n",
    "            # Save the results and metadata\n",
    "            chat_completion_metadata[\"chunk_start\"] = cursor\n",
    "            chat_completion_metadata[\"chunk_end\"] = cursor_end\n",
    "\n",
    "            tmp_prediction_df_list.append(format_message_to_df(chat_completion_message))\n",
    "            prediction_metadata_list.append(chat_completion_metadata)\n",
    "            \n",
    "            # Show usage and continue to the next chunk\n",
    "            logging.info(f\"Usage: {chat_completion_usage}\")\n",
    "            p_bar.update(chunksize)\n",
    "            cursor += chunksize\n",
    "\n",
    "    # Combine the results\n",
    "    prediction_df = (\n",
    "        # Restore the original order\n",
    "        pd.concat(tmp_prediction_df_list)\n",
    "        .reindex(indications_orig)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    prediction_metadata_df = pd.DataFrame(prediction_metadata_list)\n",
    "\n",
    "    return prediction_df, prediction_metadata_df\n",
    "\n",
    "#run_completion(train_pretty_indication, labels_pretty, chunksize=100, reduce_usage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "279ac60cf8a04ff9a4733252885c31e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/836 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing chunk 0:100 of 836\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Usage: {'completion_tokens': 1026, 'prompt_tokens': 964, 'total_tokens': 1990}\n",
      "INFO:root:Processing chunk 100:200 of 836\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Usage: {'completion_tokens': 1110, 'prompt_tokens': 973, 'total_tokens': 2083}\n",
      "INFO:root:Processing chunk 200:300 of 836\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Usage: {'completion_tokens': 1101, 'prompt_tokens': 1008, 'total_tokens': 2109}\n",
      "INFO:root:Processing chunk 300:400 of 836\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Usage: {'completion_tokens': 1051, 'prompt_tokens': 967, 'total_tokens': 2018}\n",
      "INFO:root:Processing chunk 400:500 of 836\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Usage: {'completion_tokens': 1092, 'prompt_tokens': 1012, 'total_tokens': 2104}\n",
      "INFO:root:Processing chunk 500:600 of 836\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Usage: {'completion_tokens': 1101, 'prompt_tokens': 978, 'total_tokens': 2079}\n",
      "INFO:root:Processing chunk 600:700 of 836\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Usage: {'completion_tokens': 1120, 'prompt_tokens': 1015, 'total_tokens': 2135}\n",
      "INFO:root:Processing chunk 700:800 of 836\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Usage: {'completion_tokens': 1045, 'prompt_tokens': 994, 'total_tokens': 2039}\n",
      "INFO:root:Processing chunk 800:836 of 836\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Usage: {'completion_tokens': 368, 'prompt_tokens': 514, 'total_tokens': 882}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in predictions: set()\n",
      "Not in train: set()\n",
      "({'F1-Score': 0.7071417100148766},           Urinary  Respiratory  Abdominal  Neurological  Skin Soft Tissue  \\\n",
      "F1-Score     0.98         0.96       0.83          0.88              0.87   \n",
      "\n",
      "           ENT  Orthopaedic  Other Specific  No Specific Source  Prophylaxis  \\\n",
      "F1-Score  0.79         0.87             0.3                0.34         0.94   \n",
      "\n",
      "          Uncertainty  Not Informative  \n",
      "F1-Score         0.78              0.3  )\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf5f4fabb51400c815130cf41ae8a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/587 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing chunk 0:100 of 587\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Usage: {'completion_tokens': 1134, 'prompt_tokens': 967, 'total_tokens': 2101}\n",
      "INFO:root:Processing chunk 100:200 of 587\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Usage: {'completion_tokens': 1183, 'prompt_tokens': 997, 'total_tokens': 2180}\n",
      "INFO:root:Processing chunk 200:300 of 587\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Usage: {'completion_tokens': 1216, 'prompt_tokens': 992, 'total_tokens': 2208}\n",
      "INFO:root:Processing chunk 300:400 of 587\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Usage: {'completion_tokens': 1180, 'prompt_tokens': 998, 'total_tokens': 2178}\n",
      "INFO:root:Processing chunk 400:500 of 587\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Usage: {'completion_tokens': 1210, 'prompt_tokens': 1031, 'total_tokens': 2241}\n",
      "INFO:root:Processing chunk 500:587 of 587\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Usage: {'completion_tokens': 995, 'prompt_tokens': 950, 'total_tokens': 1945}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in predictions: set()\n",
      "Not in train: set()\n",
      "({'F1-Score': 0.860384622112036},           Urinary  Respiratory  Abdominal  Neurological  Skin Soft Tissue  \\\n",
      "F1-Score     0.99          1.0       0.85           1.0              0.96   \n",
      "\n",
      "           ENT  Orthopaedic  Other Specific  No Specific Source  Prophylaxis  \\\n",
      "F1-Score  0.88         0.91            0.25                 0.6         0.96   \n",
      "\n",
      "          Uncertainty  Not Informative  \n",
      "F1-Score         0.88             0.59  )\n"
     ]
    }
   ],
   "source": [
    "locations_data = {\n",
    "    \"Oxford\": test_oxford_df,\n",
    "    \"Banbury\": test_banbury_df\n",
    "}\n",
    "\n",
    "for location, data in locations_data.items():\n",
    "    # Run completion/prediction\n",
    "    prediction_df, prediction_metadata_df = run_completion(\n",
    "        data.Indication, \n",
    "        labels_pretty,\n",
    "        model_id=model_openai_id,\n",
    "        chunksize=100, \n",
    "        reduce_usage=True\n",
    "        )\n",
    "\n",
    "    # Write the results to file\n",
    "    prediction_df.to_csv(\n",
    "        export_path / f\"predictions_zs_{location}.csv\"\n",
    "    )\n",
    "\n",
    "    prediction_metadata_df.to_csv(\n",
    "        export_path / f\"prediction_metadata_zs_{location}.csv\"\n",
    "    )\n",
    "\n",
    "    # Check for discrepancies between datasets\n",
    "    print(\"Not in predictions:\", set(data.Indication) - set(prediction_df.Indication))\n",
    "    print(\"Not in train:\", set(prediction_df.Indication) - set(data.Indication))\n",
    "\n",
    "    # Calculate the scores\n",
    "    print(score_response(prediction_df.fillna(0), data, labels_pretty))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
